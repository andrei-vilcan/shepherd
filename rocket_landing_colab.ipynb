{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Rocket Landing with Reinforcement Learning\n",
        "\n",
        "This notebook trains a neural network policy to land a rocket on a planet using REINFORCE.\n",
        "\n",
        "**Features:**\n",
        "- 2D physics simulation with gravity\n",
        "- Rocket starts in orbit, must land at target location\n",
        "- Continuous control: throttle and gimbal angle\n",
        "- GPU-accelerated training with JAX\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's check if we have GPU access and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q jax[cuda12] flax optax gymnax matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify JAX GPU setup\n",
        "import jax\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Backend: {jax.default_backend()}\")\n",
        "print(f\"Devices: {jax.devices()}\")\n",
        "\n",
        "if jax.default_backend() == 'gpu':\n",
        "    print(\"\\nâœ… GPU acceleration enabled!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Running on CPU. Go to Runtime > Change runtime type > GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Definition\n",
        "\n",
        "The rocket environment simulates:\n",
        "- A planet with gravitational pull\n",
        "- A rocket that starts in orbit\n",
        "- Target landing location on the planet surface\n",
        "- Fuel consumption and thrust physics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"JAX implementation of rocket landing environment.\"\"\"\n",
        "from typing import Any, Optional, Tuple\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import struct\n",
        "from gymnax.environments import environment, spaces\n",
        "\n",
        "# GRAVITATIONAL_CONSTANT\n",
        "G = 6.67430e-11\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvState(environment.EnvState):\n",
        "    \"\"\"State of the rocket landing environment.\"\"\"\n",
        "    x: jnp.ndarray\n",
        "    y: jnp.ndarray\n",
        "    theta: jnp.ndarray\n",
        "    dx: jnp.ndarray\n",
        "    dy: jnp.ndarray\n",
        "    omega: jnp.ndarray\n",
        "    throttle: jnp.ndarray\n",
        "    gimbal: jnp.ndarray\n",
        "    fuel: jnp.ndarray\n",
        "    target_angle: jnp.ndarray\n",
        "    time: jnp.ndarray\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvParams(environment.EnvParams):\n",
        "    \"\"\"Parameters for the rocket landing environment.\"\"\"\n",
        "    dt: float = 0.05\n",
        "    planet_radius: float = 50.\n",
        "    planet_mass: float = 5.0e14\n",
        "    rocket_max_thrust: float = 38000.0\n",
        "    rocket_max_gimbal: float = 0.3\n",
        "    rocket_mass: float = 2600.0\n",
        "    rocket_moment_of_inertia: float = 250.0\n",
        "    rocket_initial_fuel: float = 5000.0\n",
        "    rocket_fuel_consumption_rate: float = 100.\n",
        "    init_min_orbit_radius: float = 80.0\n",
        "    init_max_orbit_radius: float = 150.0\n",
        "    init_orbit_velocity_noise: float = 0.1\n",
        "    max_steps_in_episode: int = 1500\n",
        "    landing_max_speed: float = 2.0\n",
        "    landing_max_angle: float = 0.3\n",
        "    landing_max_omega: float = 0.5\n",
        "    landing_position_tolerance: float = jnp.pi/30\n",
        "    # Terminal rewards\n",
        "    reward_sl_cp_co_ls: float = 1000.0\n",
        "    reward_nsl_cp: float = 200.0\n",
        "    reward_nsl_ncp: float = 100.0\n",
        "    reward_sl_ncp_co_ls: float = 500.0\n",
        "    reward_out_of_bounds: float = -1000.0\n",
        "    reward_timeout: float = -2000.0\n",
        "    reward_retrograde_burn: float = 0.02\n",
        "\n",
        "def angle_normalize(x):\n",
        "    \"\"\"Normalize the angle - radians.\"\"\"\n",
        "    return ((x + jnp.pi) % (2 * jnp.pi)) - jnp.pi\n",
        "\n",
        "class RocketLander(environment.Environment[EnvState, EnvParams]):\n",
        "    \"\"\"JAX/Gymnax implementation of a 2D rocket landing environment.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @property\n",
        "    def default_params(self) -> EnvParams:\n",
        "        return EnvParams()\n",
        "\n",
        "    def step_env(self, key, state, action, params):\n",
        "        \"\"\"Integrate physics for one timestep.\"\"\"\n",
        "        r = jnp.sqrt(state.x**2 + state.y**2)\n",
        "        a_gravity_magnitude = G * params.planet_mass / r**2\n",
        "        a_g_x = -a_gravity_magnitude * state.x / r\n",
        "        a_g_y = -a_gravity_magnitude * state.y / r\n",
        "\n",
        "        throttle = jnp.clip(action[0], -1.0, 1.0)\n",
        "        gimbal = jnp.clip(action[1], -1.0, 1.0)\n",
        "        throttle = (throttle + 1.0) / 2.0\n",
        "        gimbal *= params.rocket_max_gimbal\n",
        "        throttle = jnp.minimum(throttle, state.fuel / (params.rocket_fuel_consumption_rate * params.dt))\n",
        "\n",
        "        thrust_angle = state.theta + gimbal\n",
        "        F_thrust = throttle * params.rocket_max_thrust\n",
        "        a_t_x = F_thrust * jnp.sin(thrust_angle) / params.rocket_mass\n",
        "        a_t_y = F_thrust * jnp.cos(thrust_angle) / params.rocket_mass\n",
        "        a_omega = F_thrust * jnp.sin(gimbal) / params.rocket_moment_of_inertia\n",
        "\n",
        "        a_x = a_g_x + a_t_x\n",
        "        a_y = a_g_y + a_t_y\n",
        "        dx = state.dx + a_x * params.dt\n",
        "        dy = state.dy + a_y * params.dt\n",
        "        omega = state.omega + a_omega * params.dt\n",
        "        x = state.x + dx * params.dt\n",
        "        y = state.y + dy * params.dt\n",
        "        theta = state.theta + omega * params.dt\n",
        "        fuel = jnp.maximum(0.0, state.fuel - throttle * params.rocket_fuel_consumption_rate * params.dt)\n",
        "\n",
        "        new_state = EnvState(\n",
        "            x=x, y=y, theta=theta, dx=dx, dy=dy, omega=omega,\n",
        "            throttle=throttle, gimbal=gimbal, fuel=fuel,\n",
        "            target_angle=state.target_angle, time=state.time + params.dt\n",
        "        )\n",
        "        done = self.is_terminal(new_state, params)\n",
        "        reward = self._compute_reward(state, new_state, params)\n",
        "        key, noise_key = jax.random.split(key)\n",
        "        obs = self.get_obs(new_state, params, noise_key)\n",
        "        return obs, new_state, reward, done, {}\n",
        "\n",
        "    def reset_env(self, key, params):\n",
        "        \"\"\"Reset environment by sampling initial orbit.\"\"\"\n",
        "        key, angle_key, orbit_radius_key, orbit_vel_noise_key, target_angle_key = jax.random.split(key, 5)\n",
        "        angle_init = jax.random.uniform(angle_key, minval=0.0, maxval=2.0 * jnp.pi)\n",
        "        orbit_radius_init = jax.random.uniform(orbit_radius_key, minval=params.init_min_orbit_radius, maxval=params.init_max_orbit_radius)\n",
        "        x_init = orbit_radius_init * jnp.cos(angle_init)\n",
        "        y_init = orbit_radius_init * jnp.sin(angle_init)\n",
        "        orbit_velocity_magnitude_init = jnp.sqrt(G * params.planet_mass / orbit_radius_init)\n",
        "        velocity_noise = params.init_orbit_velocity_noise * (2.0 * jax.random.uniform(orbit_vel_noise_key) - 1.0)\n",
        "        orbit_velocity_magnitude_init *= (1.0 + velocity_noise)\n",
        "        dx_init = -orbit_velocity_magnitude_init * jnp.sin(angle_init)\n",
        "        dy_init = orbit_velocity_magnitude_init * jnp.cos(angle_init)\n",
        "        theta_init = angle_init + (3/2) * jnp.pi\n",
        "        target_angle = jax.random.uniform(target_angle_key, minval=0.0, maxval=2.0 * jnp.pi)\n",
        "\n",
        "        state = EnvState(\n",
        "            x=x_init, y=y_init, theta=theta_init, dx=dx_init, dy=dy_init,\n",
        "            omega=jnp.array(0.0), throttle=jnp.array(0.0), gimbal=jnp.array(0.0),\n",
        "            fuel=jnp.array(params.rocket_initial_fuel), target_angle=target_angle, time=jnp.array(0)\n",
        "        )\n",
        "        return self.get_obs(state, params, key), state\n",
        "\n",
        "    def get_obs(self, state, params=None, key=None):\n",
        "        \"\"\"Get observation vector.\"\"\"\n",
        "        if params is None:\n",
        "            params = self.default_params\n",
        "        r = jnp.sqrt(state.x ** 2 + state.y ** 2)\n",
        "        altitude = r - params.planet_radius\n",
        "        positional_angle = jnp.arctan2(state.y, state.x)\n",
        "        delta_angle = angle_normalize(state.target_angle - positional_angle)\n",
        "        radial_vel = (state.x * state.dx + state.y * state.dy) / r\n",
        "        tangential_vel = (state.x * state.dy - state.y * state.dx) / r\n",
        "        theta_relative = angle_normalize(state.theta - positional_angle)\n",
        "        return jnp.array([altitude, delta_angle, radial_vel, tangential_vel, theta_relative, state.omega, state.throttle, state.gimbal, state.fuel])\n",
        "\n",
        "    def is_terminal(self, state, params):\n",
        "        \"\"\"Check whether state is terminal.\"\"\"\n",
        "        max_time = params.max_steps_in_episode * params.dt\n",
        "        timeout = state.time >= max_time\n",
        "        r = jnp.sqrt(state.x ** 2 + state.y ** 2)\n",
        "        landed = r <= params.planet_radius\n",
        "        escaped = r > params.planet_radius * 4\n",
        "        return jnp.array(landed | timeout | escaped)\n",
        "\n",
        "    def _compute_reward(self, old_state, new_state, params):\n",
        "        \"\"\"Compute reward with terminal rewards REPLACING shaping rewards.\"\"\"\n",
        "        r = jnp.sqrt(new_state.x ** 2 + new_state.y ** 2)\n",
        "        altitude = r - params.planet_radius\n",
        "        normalized_altitude = jnp.clip(altitude / (params.init_max_orbit_radius - params.planet_radius), 0.0, 2.0)\n",
        "        landed = r <= params.planet_radius\n",
        "        escaped = r > params.planet_radius * 4\n",
        "        max_time = params.max_steps_in_episode * params.dt\n",
        "        timeout = new_state.time >= max_time\n",
        "        is_terminal = landed | escaped | timeout\n",
        "\n",
        "        positional_angle = jnp.arctan2(new_state.y, new_state.x)\n",
        "        delta_angle = jnp.abs(angle_normalize(new_state.target_angle - positional_angle))\n",
        "        v_total = jnp.sqrt(new_state.dx ** 2 + new_state.dy ** 2)\n",
        "        v_tangential = (new_state.x * new_state.dy - new_state.y * new_state.dx) / r\n",
        "        theta_relative = angle_normalize(new_state.theta - positional_angle)\n",
        "\n",
        "        r_old = jnp.sqrt(old_state.x ** 2 + old_state.y ** 2)\n",
        "        normalized_altitude_old = jnp.clip((r_old - params.planet_radius) / (params.init_max_orbit_radius - params.planet_radius), 0.0, 2.0)\n",
        "        positional_angle_old = jnp.arctan2(old_state.y, old_state.x)\n",
        "        delta_angle_old = jnp.abs(angle_normalize(old_state.target_angle - positional_angle_old))\n",
        "        v_total_old = jnp.sqrt(old_state.dx ** 2 + old_state.dy ** 2)\n",
        "        v_tangential_old = (old_state.x * old_state.dy - old_state.y * old_state.dx) / r_old\n",
        "        theta_relative_old = jnp.abs(angle_normalize(old_state.theta - positional_angle_old))\n",
        "\n",
        "        slow = v_total < params.landing_max_speed\n",
        "        correct_position = delta_angle <= params.landing_position_tolerance\n",
        "        correct_orientation = jnp.abs(theta_relative) <= params.landing_max_angle\n",
        "        low_spin = jnp.abs(new_state.omega) <= params.landing_max_omega\n",
        "\n",
        "        # Terminal rewards\n",
        "        terminal_reward = jnp.array(0.0)\n",
        "        sl_cp_co_ls = landed & slow & correct_position & correct_orientation & low_spin\n",
        "        terminal_reward = jnp.where(sl_cp_co_ls, params.reward_sl_cp_co_ls, terminal_reward)\n",
        "        sl_ncp_co_ls = landed & slow & ~correct_position & correct_orientation & low_spin\n",
        "        terminal_reward = jnp.where(sl_ncp_co_ls, params.reward_sl_ncp_co_ls * jnp.exp(-delta_angle * 2.0), terminal_reward)\n",
        "        sl_nco_nls = landed & slow & ~(correct_orientation & low_spin)\n",
        "        terminal_reward = jnp.where(sl_nco_nls, 100.0 * jnp.exp(-delta_angle), terminal_reward)\n",
        "        nsl_cp = landed & ~slow & correct_position\n",
        "        terminal_reward = jnp.where(nsl_cp, params.reward_nsl_cp, terminal_reward)\n",
        "        nsl_ncp = landed & ~slow & ~correct_position\n",
        "        terminal_reward = jnp.where(nsl_ncp, params.reward_nsl_ncp, terminal_reward)\n",
        "        terminal_reward = jnp.where(escaped, params.reward_out_of_bounds, terminal_reward)\n",
        "        terminal_reward = jnp.where(timeout & ~landed & ~escaped, params.reward_timeout, terminal_reward)\n",
        "\n",
        "        # Shaping rewards\n",
        "        shaping_reward = jnp.array(0.0)\n",
        "        gamma = 0.99\n",
        "        tangential_shaping = 0.3 * (gamma * (-jnp.abs(v_tangential)) - (-jnp.abs(v_tangential_old)))\n",
        "        shaping_reward += tangential_shaping\n",
        "        tangential_weight = jnp.exp(-jnp.abs(v_tangential) * 0.5)\n",
        "        position_shaping = 0.2 * tangential_weight * (gamma * (-delta_angle) - (-delta_angle_old))\n",
        "        shaping_reward += position_shaping\n",
        "        on_target_weight = jnp.exp(-delta_angle * 3.0)\n",
        "        altitude_shaping = 0.1 * on_target_weight * (gamma * (-normalized_altitude) - (-normalized_altitude_old))\n",
        "        shaping_reward += altitude_shaping\n",
        "        ready_to_land_weight = jnp.exp(-normalized_altitude * 3.0) * jnp.exp(-v_total * 0.1)\n",
        "        orientation_shaping = 0.15 * ready_to_land_weight * (gamma * (-jnp.abs(theta_relative)) - (-theta_relative_old))\n",
        "        shaping_reward += orientation_shaping\n",
        "        omega_shaping = 0.1 * ready_to_land_weight * (gamma * (-jnp.abs(new_state.omega)) - (-jnp.abs(old_state.omega)))\n",
        "        shaping_reward += omega_shaping\n",
        "\n",
        "        # Retrograde burn reward\n",
        "        thrust_angle = new_state.theta + new_state.gimbal\n",
        "        velocity_angle = jnp.arctan2(new_state.dy, new_state.dx)\n",
        "        alignment = jnp.cos(thrust_angle - velocity_angle)\n",
        "        retrograde_alignment = (1.0 - alignment) / 2.0\n",
        "        throttle_used = new_state.throttle > 0.1\n",
        "        velocity_factor = jnp.minimum(v_total / 10.0, 1.0)\n",
        "        shaping_reward += params.reward_retrograde_burn * retrograde_alignment * throttle_used * velocity_factor\n",
        "\n",
        "        # Braking bonus\n",
        "        velocity_decrease = jnp.maximum(0.0, v_total_old - v_total)\n",
        "        shaping_reward += 0.005 * velocity_decrease\n",
        "\n",
        "        # Approach bonus\n",
        "        near_surface = normalized_altitude < 0.3\n",
        "        low_velocity = v_total < params.landing_max_speed * 2.0\n",
        "        shaping_reward += 0.05 * near_surface * low_velocity * jnp.exp(-delta_angle * 2.0)\n",
        "\n",
        "        # Spin penalty\n",
        "        high_omega = jnp.abs(new_state.omega) > 2.0\n",
        "        shaping_reward -= 0.01 * high_omega * (jnp.abs(new_state.omega) - 2.0)\n",
        "\n",
        "        return jnp.where(is_terminal, terminal_reward, shaping_reward)\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return \"RocketLander\"\n",
        "\n",
        "    @property\n",
        "    def num_actions(self):\n",
        "        return 2\n",
        "\n",
        "    def action_space(self, params=None):\n",
        "        return spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=jnp.float32)\n",
        "\n",
        "    def observation_space(self, params=None):\n",
        "        low = jnp.array([-10.0, -jnp.pi, -50.0, -50.0, -jnp.pi, -10.0, 0.0, -0.5, 0.0])\n",
        "        high = jnp.array([150.0, jnp.pi, 50.0, 50.0, jnp.pi, 10.0, 1.0, 0.5, 5000.0])\n",
        "        return spaces.Box(low, high, shape=(9,), dtype=jnp.float32)\n",
        "\n",
        "print(\"âœ… Environment defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Visualization utilities.\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "def visualize_trajectory(states, params, title=\"Rocket Landing Trajectory\", save_path=None):\n",
        "    \"\"\"Visualize the rocket trajectory around the planet.\"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "\n",
        "    # Draw planet\n",
        "    theta = np.linspace(0, 2 * np.pi, 100)\n",
        "    planet_x = params.planet_radius * np.cos(theta)\n",
        "    planet_y = params.planet_radius * np.sin(theta)\n",
        "    ax.fill(planet_x, planet_y, color='#3d5a80', alpha=0.8, label='Planet')\n",
        "    ax.plot(planet_x, planet_y, color='#1d3557', linewidth=2)\n",
        "\n",
        "    # Extract trajectory\n",
        "    xs = np.array([float(s.x) for s in states])\n",
        "    ys = np.array([float(s.y) for s in states])\n",
        "    throttles = np.array([float(s.throttle) for s in states])\n",
        "\n",
        "    # Draw trajectory colored by throttle\n",
        "    points = np.array([xs, ys]).T.reshape(-1, 1, 2)\n",
        "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "    norm = plt.Normalize(0, 1)\n",
        "    lc = LineCollection(segments, cmap='YlOrRd', norm=norm, linewidth=2, alpha=0.8)\n",
        "    lc.set_array(throttles[:-1])\n",
        "    ax.add_collection(lc)\n",
        "    cbar = plt.colorbar(lc, ax=ax, label='Throttle')\n",
        "    cbar.ax.yaxis.set_tick_params(color='white')\n",
        "    cbar.ax.yaxis.label.set_color('white')\n",
        "    plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color='white')\n",
        "\n",
        "    # Markers\n",
        "    ax.scatter(xs[0], ys[0], s=100, c='green', marker='o', zorder=5, label='Start')\n",
        "    ax.scatter(xs[-1], ys[-1], s=100, c='red', marker='x', zorder=5, label='End')\n",
        "\n",
        "    # Target\n",
        "    target_angle = float(states[0].target_angle)\n",
        "    target_x = params.planet_radius * np.cos(target_angle)\n",
        "    target_y = params.planet_radius * np.sin(target_angle)\n",
        "    ax.scatter(target_x, target_y, s=200, c='yellow', marker='*', zorder=5, edgecolors='black', label='Target')\n",
        "\n",
        "    # Rocket orientation arrows\n",
        "    n_arrows = min(30, len(states))\n",
        "    arrow_indices = np.linspace(0, len(states) - 1, n_arrows, dtype=int)\n",
        "    for idx in arrow_indices:\n",
        "        s = states[idx]\n",
        "        x, y = float(s.x), float(s.y)\n",
        "        theta = float(s.theta)\n",
        "        dx = np.sin(theta) * 8\n",
        "        dy = np.cos(theta) * 8\n",
        "        ax.arrow(x, y, dx, dy, head_width=4, head_length=2, fc='#cccccc', ec='white', alpha=0.9)\n",
        "\n",
        "    # Styling\n",
        "    fov = params.init_max_orbit_radius * 1.2\n",
        "    ax.set_xlim(-fov, fov)\n",
        "    ax.set_ylim(-fov, fov)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlabel('X Position')\n",
        "    ax.set_ylabel('Y Position')\n",
        "    ax.set_title(title)\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_facecolor('#0d1b2a')\n",
        "    fig.patch.set_facecolor('#0d1b2a')\n",
        "    ax.tick_params(colors='white')\n",
        "    ax.xaxis.label.set_color('white')\n",
        "    ax.yaxis.label.set_color('white')\n",
        "    ax.title.set_color('white')\n",
        "    ax.legend(facecolor='#1b263b', edgecolor='white', labelcolor='white')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, facecolor=fig.get_facecolor())\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "    return fig\n",
        "\n",
        "print(\"âœ… Visualization utilities defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Policy and Training\n",
        "\n",
        "We use REINFORCE with a Gaussian policy to train the rocket landing agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Training components.\"\"\"\n",
        "import jax.random as jr\n",
        "from jax.random import PRNGKey\n",
        "import optax\n",
        "\n",
        "# Create the environment\n",
        "env = RocketLander()\n",
        "env_params = env.default_params\n",
        "\n",
        "def initialize_mlp(layer_sizes, key, scale=1e-2):\n",
        "    \"\"\"Initialize MLP parameters.\"\"\"\n",
        "    keys = jr.split(key, 2 * len(layer_sizes))\n",
        "    params = []\n",
        "    for i in range(len(layer_sizes) - 1):\n",
        "        input_size, output_size = layer_sizes[i], layer_sizes[i + 1]\n",
        "        W = jr.normal(keys[2 * i], (input_size, output_size)) * scale\n",
        "        b = jr.normal(keys[2 * i + 1], (output_size,)) * scale\n",
        "        params.append((W, b))\n",
        "    return params\n",
        "\n",
        "def policy(params, x):\n",
        "    \"\"\"Gaussian policy network.\"\"\"\n",
        "    for (W, b) in params[:-1]:\n",
        "        x = jnp.dot(x, W) + b\n",
        "        x = jax.nn.relu(x)\n",
        "    W, b = params[-1]\n",
        "    output = jnp.dot(x, W) + b\n",
        "    action_dim = output.shape[-1] // 2\n",
        "    mean = jnp.tanh(output[:action_dim])\n",
        "    log_std = jnp.clip(output[action_dim:], -2.0, 0.5)\n",
        "    return mean, log_std\n",
        "\n",
        "def get_action(params, x, key):\n",
        "    \"\"\"Sample an action from the Gaussian policy.\"\"\"\n",
        "    mean, log_std = policy(params, x)\n",
        "    std = jnp.exp(log_std)\n",
        "    noise = jr.normal(key, shape=mean.shape)\n",
        "    action = jnp.clip(mean + std * noise, -1.0, 1.0)\n",
        "    return action, mean, log_std\n",
        "\n",
        "def get_log_prob(params, x, action):\n",
        "    \"\"\"Compute log probability of action under the Gaussian policy.\"\"\"\n",
        "    mean, log_std = policy(params, x)\n",
        "    std = jnp.exp(log_std)\n",
        "    return -0.5 * jnp.sum(((action - mean) / std) ** 2 + 2 * log_std + jnp.log(2 * jnp.pi))\n",
        "\n",
        "@jax.jit\n",
        "def update_delta(delta, grad_theta):\n",
        "    \"\"\"Update the parameter gradients.\"\"\"\n",
        "    return jax.tree.map(lambda x, y: x + y, delta, grad_theta), None\n",
        "\n",
        "def rollout(params, env_params, rng_input, steps_in_episode):\n",
        "    \"\"\"Rollout an episode using the policy.\"\"\"\n",
        "    rng_reset, rng_episode = jr.split(rng_input)\n",
        "    obs, state = env.reset_env(rng_reset, env_params)\n",
        "\n",
        "    def policy_step(state_input, tmp):\n",
        "        obs, state, rng = state_input\n",
        "        rng, rng_action, rng_step = jr.split(rng, 3)\n",
        "        action, mean, log_std = get_action(params, obs, rng_action)\n",
        "        next_obs, next_state, reward, done, _ = env.step_env(rng_step, state, action, env_params)\n",
        "        carry = [next_obs, next_state, rng]\n",
        "        return carry, [obs, state, action, reward, next_obs, done]\n",
        "\n",
        "    _, scan_out = jax.lax.scan(policy_step, [obs, state, rng_episode], (), length=steps_in_episode)\n",
        "    return scan_out\n",
        "\n",
        "jit_rollout = jax.jit(rollout, static_argnums=3)\n",
        "\n",
        "def loss_REINFORCE(params, obs, action, reward, baseline, gamma=0.99):\n",
        "    \"\"\"Compute REINFORCE loss with baseline.\"\"\"\n",
        "    def trajectory_gradients(reward, obs, action, baseline, delta):\n",
        "        G_init = 0.0\n",
        "        def step(carry, variables):\n",
        "            G, delta = carry\n",
        "            r, obs, action, b = variables\n",
        "            G = gamma * G + r\n",
        "            advantage = G - b\n",
        "            def neg_log_prob(params):\n",
        "                return -get_log_prob(params, obs, action)\n",
        "            grad_delta = jax.grad(neg_log_prob)(params)\n",
        "            grad_delta = jax.tree.map(lambda gd: gd * advantage, grad_delta)\n",
        "            delta, _ = update_delta(delta, grad_delta)\n",
        "            return (G, delta), G\n",
        "        variables = (reward[::-1], obs[::-1], action[::-1], baseline[::-1])\n",
        "        (_, delta), Gt = jax.lax.scan(step, (G_init, delta), variables)\n",
        "        return delta, Gt\n",
        "\n",
        "    parallel_trajectory_gradients = jax.vmap(trajectory_gradients, in_axes=(0, 0, 0, None, None))\n",
        "    delta = jax.tree.map(lambda t: jnp.zeros(t.shape), params)\n",
        "    deltas, Gs = parallel_trajectory_gradients(reward, obs, action, baseline, delta)\n",
        "    delta, _ = jax.lax.scan(update_delta, delta, deltas)\n",
        "    return delta, jnp.array(Gs)\n",
        "\n",
        "loss_REINFORCE = jax.jit(loss_REINFORCE)\n",
        "\n",
        "def mean_baseline(Gs):\n",
        "    \"\"\"Compute constant baseline as mean of discounted returns.\"\"\"\n",
        "    T = Gs.shape[1]\n",
        "    return jnp.ones((T,)) * jnp.mean(Gs)\n",
        "\n",
        "print(\"âœ… Training components defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    num_iters=5000,\n",
        "    steps_in_episode=1500,\n",
        "    lr=0.0002,\n",
        "    gamma=0.995,\n",
        "    n_batches=128,\n",
        "    hidden_size=256,\n",
        "    n_hidden_layers=3,\n",
        "    visualize_every=500,\n",
        "    seed=42,\n",
        "    grad_clip=0.5,\n",
        "):\n",
        "    \"\"\"Train the rocket landing policy using REINFORCE.\"\"\"\n",
        "    key = PRNGKey(seed)\n",
        "    \n",
        "    # Network architecture\n",
        "    obs_dim = 9\n",
        "    action_dim = 2\n",
        "    hidden_layers = tuple([hidden_size] * n_hidden_layers)\n",
        "    layer_sizes = (obs_dim,) + hidden_layers + (2 * action_dim,)\n",
        "    \n",
        "    # Initialize\n",
        "    key, subkey = jr.split(key)\n",
        "    params = initialize_mlp(layer_sizes, key=subkey)\n",
        "    optim = optax.chain(optax.clip_by_global_norm(grad_clip), optax.adam(learning_rate=lr))\n",
        "    opt_state = optim.init(params)\n",
        "    Gs = jnp.zeros((n_batches, steps_in_episode))\n",
        "    \n",
        "    key, subkey = jr.split(key)\n",
        "    iter_keys = jr.split(subkey, num_iters)\n",
        "    parallel_rollout = jax.vmap(rollout, in_axes=(None, None, 0, None))\n",
        "    \n",
        "    def step(carry, key):\n",
        "        params, opt_state, Gs = carry\n",
        "        keys = jr.split(key, n_batches)\n",
        "        obs, state, action, reward, next_obs, done = parallel_rollout(params, env_params, keys, steps_in_episode)\n",
        "        baseline = mean_baseline(Gs)\n",
        "        delta, Gs_new = loss_REINFORCE(params, obs, action, reward, baseline, gamma)\n",
        "        updates, opt_state = optim.update(delta, opt_state, params)\n",
        "        new_params = optax.apply_updates(params, updates)\n",
        "        mean_reward = jnp.mean(jnp.sum(reward, axis=-1))\n",
        "        return (new_params, opt_state, Gs_new), mean_reward\n",
        "\n",
        "    step = jax.jit(step)\n",
        "    \n",
        "    history = []\n",
        "    current_params = params\n",
        "    current_opt_state = opt_state\n",
        "    current_Gs = Gs\n",
        "    \n",
        "    print(f\"ðŸš€ Starting training for {num_iters} iterations...\")\n",
        "    print(f\"   Network: {layer_sizes}\")\n",
        "    print(f\"   Batch size: {n_batches}, Steps: {steps_in_episode}\")\n",
        "    print(f\"   LR: {lr}, Gamma: {gamma}\")\n",
        "    print()\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "        (current_params, current_opt_state, current_Gs), mean_reward = step(\n",
        "            (current_params, current_opt_state, current_Gs), iter_keys[i]\n",
        "        )\n",
        "        history.append(float(mean_reward))\n",
        "        \n",
        "        if (i + 1) % 100 == 0:\n",
        "            avg_reward = jnp.mean(jnp.array(history[-100:]))\n",
        "            print(f\"Iteration {i + 1}/{num_iters} | Avg Reward (last 100): {avg_reward:.2f}\")\n",
        "        \n",
        "        if (i + 1) % visualize_every == 0:\n",
        "            print(f\"\\nðŸ“Š Visualizing at iteration {i + 1}...\")\n",
        "            key, vis_key = jr.split(key)\n",
        "            visualize_policy(current_params, vis_key, title=f\"Iteration {i + 1}\")\n",
        "    \n",
        "    print(\"\\nâœ… Training complete!\")\n",
        "    key, vis_key = jr.split(key)\n",
        "    visualize_policy(current_params, vis_key, title=\"Final Policy\")\n",
        "    \n",
        "    # Plot training curve\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(history, alpha=0.3, label='Episode Reward')\n",
        "    window = min(100, len(history) // 10)\n",
        "    if window > 1:\n",
        "        smoothed = jnp.convolve(jnp.array(history), jnp.ones(window) / window, mode='valid')\n",
        "        plt.plot(range(window - 1, len(history)), smoothed, label=f'Smoothed (window={window})')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Mean Episode Reward')\n",
        "    plt.title('Training Progress')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return current_params, history\n",
        "\n",
        "\n",
        "def visualize_policy(params, key, title=\"Policy Trajectory\"):\n",
        "    \"\"\"Run a single episode and visualize the trajectory.\"\"\"\n",
        "    obs, state, action, reward, next_obs, done = rollout(params, env_params, key, env_params.max_steps_in_episode)\n",
        "    states = []\n",
        "    for i in range(len(state.x)):\n",
        "        s = EnvState(\n",
        "            x=state.x[i], y=state.y[i], theta=state.theta[i],\n",
        "            dx=state.dx[i], dy=state.dy[i], omega=state.omega[i],\n",
        "            throttle=state.throttle[i], gimbal=state.gimbal[i],\n",
        "            fuel=state.fuel[i], target_angle=state.target_angle[i], time=state.time[i],\n",
        "        )\n",
        "        states.append(s)\n",
        "        if done[i]:\n",
        "            break\n",
        "    \n",
        "    total_reward = float(jnp.sum(reward))\n",
        "    print(f\"  Total reward: {total_reward:.2f}, Steps: {len(states)}\")\n",
        "    visualize_trajectory(states, env_params, title=f\"{title} (R={total_reward:.1f})\")\n",
        "\n",
        "print(\"âœ… Training function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ® Run Training!\n",
        "\n",
        "Adjust the parameters below and run training. With GPU, this should be quite fast!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the agent!\n",
        "# Adjust num_iters for longer/shorter training\n",
        "\n",
        "params, history = train(\n",
        "    num_iters=10000,          # Number of training iterations\n",
        "    steps_in_episode=1500,    # Max steps per episode\n",
        "    lr=0.0002,                # Learning rate\n",
        "    gamma=0.995,              # Discount factor\n",
        "    n_batches=128,            # Parallel episodes per iteration\n",
        "    hidden_size=256,          # Hidden layer size\n",
        "    n_hidden_layers=3,        # Number of hidden layers\n",
        "    visualize_every=1000,     # Visualize every N iterations\n",
        "    seed=42,\n",
        "    grad_clip=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Trained Policy\n",
        "\n",
        "Run multiple episodes to see how the trained agent performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained policy multiple times\n",
        "key = jr.PRNGKey(123)\n",
        "\n",
        "for i in range(5):\n",
        "    key, subkey = jr.split(key)\n",
        "    print(f\"\\n--- Episode {i+1} ---\")\n",
        "    visualize_policy(params, subkey, title=f\"Test Episode {i+1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save/Load Model\n",
        "\n",
        "You can save the trained parameters to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model (optional)\n",
        "import pickle\n",
        "\n",
        "# Flatten params for saving\n",
        "params_np = jax.tree.map(lambda x: np.array(x), params)\n",
        "\n",
        "with open('rocket_lander_params.pkl', 'wb') as f:\n",
        "    pickle.dump(params_np, f)\n",
        "\n",
        "print(\"Model saved to rocket_lander_params.pkl\")\n",
        "\n",
        "# To load:\n",
        "# with open('rocket_lander_params.pkl', 'rb') as f:\n",
        "#     params_loaded = pickle.load(f)\n",
        "# params_loaded = jax.tree.map(lambda x: jnp.array(x), params_loaded)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

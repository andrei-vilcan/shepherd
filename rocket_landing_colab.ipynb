{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Rocket Landing with REINFORCE\n",
        "\n",
        "This notebook trains an AI agent to land a rocket on a planet using the REINFORCE algorithm.\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q jax[cuda12] flax optax gymnax matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify JAX is using GPU\n",
        "import jax\n",
        "print(f\"JAX devices: {jax.devices()}\")\n",
        "print(f\"Default backend: {jax.default_backend()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Definition\n",
        "\n",
        "The rocket landing environment simulates a 2D orbital mechanics scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"JAX implementation of rocket landing environment.\"\"\"\n",
        "from typing import Any, Optional, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import struct\n",
        "from gymnax.environments import environment, spaces\n",
        "\n",
        "# GRAVITATIONAL_CONSTANT\n",
        "G = 6.67430e-11\n",
        "\n",
        "\n",
        "def angle_normalize(x: jax.Array) -> jax.Array:\n",
        "    \"\"\"Normalize the angle - radians.\"\"\"\n",
        "    return ((x + jnp.pi) % (2 * jnp.pi)) - jnp.pi\n",
        "\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvState(environment.EnvState):\n",
        "    \"\"\"State of the rocket landing environment.\"\"\"\n",
        "    # position\n",
        "    x: jnp.ndarray\n",
        "    y: jnp.ndarray\n",
        "    theta: jnp.ndarray\n",
        "\n",
        "    # velocity\n",
        "    dx: jnp.ndarray\n",
        "    dy: jnp.ndarray\n",
        "    omega: jnp.ndarray\n",
        "\n",
        "    # engine\n",
        "    throttle: jnp.ndarray\n",
        "    gimbal: jnp.ndarray\n",
        "\n",
        "    # fuel\n",
        "    fuel: jnp.ndarray\n",
        "\n",
        "    # target landing angle\n",
        "    target_angle: jnp.ndarray\n",
        "\n",
        "    # timestep\n",
        "    time: jnp.ndarray\n",
        "\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvParams(environment.EnvParams):\n",
        "    \"\"\"Parameters for the rocket landing environment.\"\"\"\n",
        "    max_steps_in_episode: int = 1500\n",
        "\n",
        "    dt: float = 0.05\n",
        "\n",
        "    # planet properties\n",
        "    planet_radius: float = 50.0\n",
        "    planet_mass: float = 5.0e14\n",
        "\n",
        "    # rocket properties\n",
        "    rocket_max_thrust: float = 38000.0\n",
        "    rocket_max_gimbal: float = 0.3\n",
        "    rocket_mass: float = 2600.0\n",
        "    rocket_moment_of_inertia: float = 250.0\n",
        "    rocket_initial_fuel: float = 5000.0\n",
        "    rocket_fuel_consumption_rate: float = 100.\n",
        "\n",
        "    # initialization parameters\n",
        "    init_min_orbit_radius: float = 80.0\n",
        "    init_max_orbit_radius: float = 150.0\n",
        "    init_orbit_velocity_noise: float = 0.1\n",
        "\n",
        "    # good landing thresholds\n",
        "    landing_max_speed: float = 2.0\n",
        "    landing_max_angle: float = 0.3\n",
        "    landing_position_tolerance: float = jnp.pi/30\n",
        "    landing_max_omega: float = 0.5\n",
        "\n",
        "    # terminal rewards\n",
        "    reward_landed: float = 100.0\n",
        "    reward_sl_cp_co_ls: float = 1000.0\n",
        "    reward_nsl_cp: float = 200.0\n",
        "    reward_nsl_ncp: float = 100.0\n",
        "    reward_sl_ncp_co_ls: float = 500.0\n",
        "    reward_out_of_fuel: float = -50.0\n",
        "    reward_out_of_bounds: float = -1000.0\n",
        "    reward_timeout: float = -2000.0\n",
        "\n",
        "    # non-terminal rewards\n",
        "    reward_altitude_factor: float = 0.001\n",
        "    reward_angular_position_factor: float = 0.0003\n",
        "    reward_radial_velocity_factor: float = 0.0003\n",
        "    reward_tangential_velocity_factor: float = 0.0005\n",
        "    reward_angle_factor: float = 0.0003\n",
        "    reward_angular_velocity_factor: float = 0.00005\n",
        "    reward_fuel_penalty: float = 0.0  # Disabled to encourage engine use\n",
        "    reward_low_velocity_near_surface: float = 0.0005\n",
        "    reward_deceleration_with_fuel: float = 0.0003\n",
        "    reward_retrograde_orientation: float = 0.0005\n",
        "    reward_retrograde_burn: float = 0.02  # Reward for firing engine while aligned for braking\n",
        "\n",
        "\n",
        "class RocketLander(environment.Environment[EnvState, EnvParams]):\n",
        "    \"\"\"JAX/Gymnax implementation of a 2D rocket landing environment.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @property\n",
        "    def default_params(self) -> EnvParams:\n",
        "        return EnvParams()\n",
        "\n",
        "    def step_env(\n",
        "            self,\n",
        "            key: jax.Array,\n",
        "            state: EnvState,\n",
        "            action: jax.Array,\n",
        "            params: EnvParams,\n",
        "    ) -> Tuple[jax.Array, EnvState, jax.Array, jax.Array, dict]:\n",
        "        \"\"\"Integrate\"\"\"\n",
        "\n",
        "        ### deterministic step\n",
        "\n",
        "        # gravity\n",
        "        r = jnp.sqrt(state.x**2 + state.y**2)\n",
        "        F_gravity_magnitude = G * params.planet_mass * params.rocket_mass / r**2\n",
        "        # gravitational acceleration (a = F/m)\n",
        "        a_gravity_magnitude = F_gravity_magnitude / params.rocket_mass\n",
        "\n",
        "        a_gravity_magnitude = G * params.planet_mass / r**2\n",
        "\n",
        "        a_g_x = -a_gravity_magnitude * state.x / r\n",
        "        a_g_y = -a_gravity_magnitude * state.y / r\n",
        "\n",
        "        ### control step\n",
        "\n",
        "        # throttle and gimbal\n",
        "        throttle_action = action[0]\n",
        "        gimbal_action = action[1]\n",
        "\n",
        "        throttle = jnp.clip(throttle_action, -1.0, 1.0)\n",
        "        gimbal = jnp.clip(gimbal_action, -1.0, 1.0)\n",
        "\n",
        "        throttle = (throttle + 1.0)/2.0\n",
        "        gimbal *= params.rocket_max_gimbal\n",
        "\n",
        "        # can only get as much throttle as we have fuel / consumption\n",
        "        throttle = jnp.minimum(throttle, state.fuel / (params.rocket_fuel_consumption_rate * params.dt))\n",
        "\n",
        "        # thrust\n",
        "        thrust_angle = state.theta + gimbal\n",
        "        F_thrust = throttle * params.rocket_max_thrust\n",
        "        # acceleration (a = F/m)\n",
        "        a_t_x = F_thrust * jnp.sin(thrust_angle) / params.rocket_mass\n",
        "        a_t_y = F_thrust * jnp.cos(thrust_angle) / params.rocket_mass\n",
        "        # torque\n",
        "        F_torque = F_thrust * jnp.sin(gimbal)\n",
        "        a_omega = F_torque / params.rocket_moment_of_inertia\n",
        "\n",
        "        ### new state calculation\n",
        "        # directional sum of gravitational and thrusticular accelerations\n",
        "        a_x = a_g_x + a_t_x\n",
        "        a_y = a_g_y + a_t_y\n",
        "\n",
        "        dx = state.dx + a_x * params.dt\n",
        "        dy = state.dy + a_y * params.dt\n",
        "        omega = state.omega + a_omega * params.dt\n",
        "\n",
        "        x = state.x + dx * params.dt\n",
        "        y = state.y + dy * params.dt\n",
        "        theta = state.theta + omega * params.dt\n",
        "\n",
        "        fuel = state.fuel - throttle * params.rocket_fuel_consumption_rate * params.dt\n",
        "        fuel = jnp.maximum(0.0, fuel)  # safety\n",
        "\n",
        "        new_state = EnvState(\n",
        "            x=x,\n",
        "            y=y,\n",
        "            theta=theta,\n",
        "            dx=dx,\n",
        "            dy=dy,\n",
        "            omega=omega,\n",
        "            throttle=throttle,\n",
        "            gimbal=gimbal,\n",
        "            fuel=fuel,\n",
        "            target_angle=state.target_angle,\n",
        "            time=state.time+params.dt\n",
        "        )\n",
        "        done = self.is_terminal(new_state, params)\n",
        "        reward = self._compute_reward(state, new_state, params)\n",
        "        key, noise_key = jax.random.split(key)\n",
        "        obs = self.get_obs(new_state, params, noise_key)\n",
        "        return obs, new_state, reward, done, {}\n",
        "\n",
        "    def reset_env(\n",
        "            self, key: jax.Array, params: EnvParams\n",
        "    ) -> Tuple[jax.Array, EnvState]:\n",
        "        \"\"\"Reset environment by sampling.\"\"\"\n",
        "        key, angle_key, orbit_radius_key, orbit_vel_noise_key, target_angle_key = jax.random.split(key, 5)\n",
        "\n",
        "        ### rocket\n",
        "        # initial position and velocity\n",
        "        angle_init = jax.random.uniform(angle_key, minval=0.0, maxval=2.0 * jnp.pi)\n",
        "\n",
        "        orbit_radius_init = jax.random.uniform(orbit_radius_key, minval=params.init_min_orbit_radius, maxval=params.init_max_orbit_radius)\n",
        "\n",
        "        x_init = orbit_radius_init * jnp.cos(angle_init)\n",
        "        y_init = orbit_radius_init * jnp.sin(angle_init)\n",
        "\n",
        "        orbit_velocity_magnitude_init = jnp.sqrt(G * params.planet_mass / orbit_radius_init)\n",
        "        # Add noise scaled by the noise parameter\n",
        "        velocity_noise = params.init_orbit_velocity_noise * (2.0 * jax.random.uniform(orbit_vel_noise_key) - 1.0)\n",
        "        orbit_velocity_magnitude_init *= (1.0 + velocity_noise)\n",
        "\n",
        "        dx_init = -orbit_velocity_magnitude_init * jnp.sin(angle_init)  # negative for counterclockwise orbit\n",
        "        dy_init = orbit_velocity_magnitude_init * jnp.cos(angle_init)\n",
        "\n",
        "        # initial angle and angular velocity\n",
        "        theta_init = angle_init + (3/2)*jnp.pi  # rocket always orbiting \"backwards\" (thruster facing direction of motion)\n",
        "        \n",
        "        # Tidal locking: rocket rotates at the same rate as it orbits, keeping the same face toward the planet\n",
        "        # For a counterclockwise orbit, the positional angle increases at rate: omega_orbit = v / r\n",
        "        # The rocket must rotate at this same rate to maintain orientation relative to the planet\n",
        "        omega_init = orbit_velocity_magnitude_init / orbit_radius_init\n",
        "\n",
        "        target_angle = jax.random.uniform(target_angle_key, minval=0.0, maxval=2.0 * jnp.pi)\n",
        "\n",
        "        state = EnvState(\n",
        "            x=x_init,\n",
        "            y=y_init,\n",
        "            theta=theta_init,\n",
        "            dx=dx_init,\n",
        "            dy=dy_init,\n",
        "            omega=omega_init,\n",
        "            throttle=jnp.array(0.0),\n",
        "            gimbal=jnp.array(0.0),\n",
        "            fuel=jnp.array(params.rocket_initial_fuel),\n",
        "            target_angle=target_angle,\n",
        "            time=jnp.array(0)\n",
        "        )\n",
        "\n",
        "        return self.get_obs(state, params, key), state\n",
        "\n",
        "    def get_obs(self, state: EnvState, params: EnvParams=None, key: jax.Array=None) -> jax.Array:\n",
        "        \"\"\"Return observation from state.\"\"\"\n",
        "\n",
        "        if params is None:  # safety - always provide params\n",
        "            params = self.default_params\n",
        "\n",
        "        r = jnp.sqrt(state.x ** 2 + state.y ** 2)\n",
        "        altitude = r - params.planet_radius\n",
        "\n",
        "        positional_angle = jnp.arctan2(state.y, state.x)\n",
        "\n",
        "        delta_angle = state.target_angle - positional_angle\n",
        "        delta_angle = angle_normalize(delta_angle)\n",
        "\n",
        "        radial_vel = (state.x * state.dx + state.y * state.dy) / r\n",
        "        tangential_vel = (state.x * state.dy - state.y * state.dx) / r\n",
        "\n",
        "        theta_relative = state.theta - positional_angle\n",
        "        theta_relative = angle_normalize(theta_relative)\n",
        "\n",
        "        # observation\n",
        "        obs = jnp.array(\n",
        "            [\n",
        "                # polar coordinates (position)\n",
        "                altitude,\n",
        "                delta_angle,\n",
        "                # polar coordinates (velocity)\n",
        "                radial_vel,\n",
        "                tangential_vel,\n",
        "                # rotational values\n",
        "                theta_relative,\n",
        "                state.omega,\n",
        "                # engine\n",
        "                state.throttle,\n",
        "                state.gimbal,\n",
        "                state.fuel\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def is_terminal(self, state: EnvState, params: EnvParams) -> jax.Array:\n",
        "        \"\"\"Check whether state is terminal.\"\"\"\n",
        "\n",
        "        # timeout based on time in seconds (max_steps * dt)\n",
        "        max_time = params.max_steps_in_episode * params.dt\n",
        "        timeout = state.time >= max_time\n",
        "\n",
        "        r = jnp.sqrt(state.x ** 2 + state.y ** 2)\n",
        "\n",
        "        landed = r <= params.planet_radius\n",
        "\n",
        "        escaped = r > params.planet_radius * 4\n",
        "\n",
        "        done = landed | timeout | escaped\n",
        "        return jnp.array(done)\n",
        "\n",
        "    def _compute_reward(self, old_state: EnvState, new_state: EnvState, params: EnvParams) -> jax.Array:\n",
        "        \"\"\"Compute reward.\"\"\"\n",
        "        \n",
        "        ### preliminary calculations\n",
        "\n",
        "        ## new state\n",
        "        r = jnp.sqrt(new_state.x ** 2 + new_state.y ** 2)\n",
        "        altitude = r - params.planet_radius\n",
        "        normalized_altitude = jnp.clip(altitude / (params.init_max_orbit_radius - params.planet_radius), 0.0, 2.0)\n",
        "\n",
        "        # terminal conditions\n",
        "        landed = r <= params.planet_radius\n",
        "        escaped = r > params.planet_radius * 4\n",
        "        max_time = params.max_steps_in_episode * params.dt\n",
        "        timeout = new_state.time >= max_time\n",
        "        is_terminal = landed | escaped | timeout\n",
        "\n",
        "        # angular distance from target landing spot\n",
        "        positional_angle = jnp.arctan2(new_state.y, new_state.x)\n",
        "        delta_angle = jnp.abs(angle_normalize(new_state.target_angle - positional_angle))\n",
        "\n",
        "        # various velocities\n",
        "        v_total = jnp.sqrt(new_state.dx ** 2 + new_state.dy ** 2)\n",
        "        v_radial = (new_state.x * new_state.dx + new_state.y * new_state.dy) / r\n",
        "        v_tangential = (new_state.x * new_state.dy - new_state.y * new_state.dx) / r\n",
        "\n",
        "        # get rocket orientation relative to surface\n",
        "        theta_relative = angle_normalize(new_state.theta - positional_angle)\n",
        "\n",
        "        ## old state (same calculations)\n",
        "        r_old = jnp.sqrt(old_state.x ** 2 + old_state.y ** 2)\n",
        "        altitude_old = r_old - params.planet_radius\n",
        "        normalized_altitude_old = jnp.clip(altitude_old / (params.init_max_orbit_radius - params.planet_radius), 0.0, 2.0)\n",
        "\n",
        "        positional_angle_old = jnp.arctan2(old_state.y, old_state.x)\n",
        "        delta_angle_old = jnp.abs(angle_normalize(old_state.target_angle - positional_angle_old))\n",
        "\n",
        "        v_total_old = jnp.sqrt(old_state.dx ** 2 + old_state.dy ** 2)\n",
        "        v_radial_old = (old_state.x * old_state.dx + old_state.y * old_state.dy) / r_old\n",
        "        v_tangential_old = (old_state.x * old_state.dy - old_state.y * old_state.dx) / r_old\n",
        "\n",
        "        theta_relative_old = jnp.abs(angle_normalize(old_state.theta - positional_angle_old))\n",
        "\n",
        "        ## landing conditions\n",
        "        slow = v_total < params.landing_max_speed\n",
        "        correct_position = delta_angle <= params.landing_position_tolerance\n",
        "        correct_orientation = jnp.abs(theta_relative) <= params.landing_max_angle\n",
        "        low_spin = jnp.abs(new_state.omega) <= params.landing_max_omega\n",
        "\n",
        "        ### terminal rewards\n",
        "\n",
        "        terminal_reward = jnp.array(0.0)\n",
        "\n",
        "        # ideal landing: slow, correct position, correct orientation, low spin\n",
        "        sl_cp_co_ls = landed & slow & correct_position & correct_orientation & low_spin\n",
        "        terminal_reward = jnp.where(sl_cp_co_ls, params.reward_sl_cp_co_ls, terminal_reward)\n",
        "\n",
        "        # ideal landing at incorrect position\n",
        "        sl_ncp_co_ls = landed & slow & ~correct_position & correct_orientation & low_spin\n",
        "        terminal_reward = jnp.where(sl_ncp_co_ls, params.reward_sl_ncp_co_ls * jnp.exp(-delta_angle * 2.0), terminal_reward)\n",
        "\n",
        "        # slow landing but wrong orientation/spin\n",
        "        sl_nco_nls = landed & slow & ~(correct_orientation & low_spin)\n",
        "        slow_reward = 100.0 * jnp.exp(-delta_angle)\n",
        "        terminal_reward = jnp.where(sl_nco_nls, slow_reward, terminal_reward)\n",
        "\n",
        "        # non-ideal (fast) landing at correct position\n",
        "        nsl_cp = landed & ~slow & correct_position\n",
        "        terminal_reward = jnp.where(nsl_cp, params.reward_nsl_cp, terminal_reward)\n",
        "\n",
        "        # non-ideal (fast) landing at incorrect position\n",
        "        nsl_ncp = landed & ~slow & ~correct_position\n",
        "        terminal_reward = jnp.where(nsl_ncp, params.reward_nsl_ncp, terminal_reward)\n",
        "\n",
        "        # escaped bounds\n",
        "        terminal_reward = jnp.where(escaped, params.reward_out_of_bounds, terminal_reward)\n",
        "\n",
        "        # timeout - heavily penalized to discourage staying in orbit\n",
        "        terminal_reward = jnp.where(timeout & ~landed & ~escaped, params.reward_timeout, terminal_reward)\n",
        "\n",
        "        ### non-terminal rewards\n",
        "        \n",
        "        shaping_reward = jnp.array(0.0)\n",
        "\n",
        "        # reward retrograde burn\n",
        "        thrust_angle = new_state.theta + new_state.gimbal\n",
        "        velocity_angle = jnp.arctan2(new_state.dy, new_state.dx)\n",
        "        alignment = jnp.cos(thrust_angle - velocity_angle)\n",
        "        retrograde_alignment = (1.0 - alignment) / 2.0\n",
        "        throttle_used = new_state.throttle > 0.1\n",
        "        velocity_factor = jnp.minimum(v_total / 10.0, 1.0)\n",
        "        retrograde_burn_reward = params.reward_retrograde_burn * retrograde_alignment * throttle_used * velocity_factor\n",
        "        shaping_reward += retrograde_burn_reward\n",
        "\n",
        "        # reward tangential velocity -> 0\n",
        "        tangential_velocity_change = jnp.abs(v_tangential) - jnp.abs(v_tangential_old)\n",
        "        shaping_reward += -0.3 * tangential_velocity_change\n",
        "\n",
        "        # if delta_angle -> 0, reward altitude -> 0\n",
        "        delta_angle_factor = jnp.exp(-delta_angle * 3)\n",
        "        altitude_change = normalized_altitude - normalized_altitude_old\n",
        "        shaping_reward += -1.0 * delta_angle_factor * altitude_change\n",
        "\n",
        "        # if landing (low altitude and low velocity), reward theta_relative -> 0\n",
        "        near_landing_factor = jnp.exp(-3.0 * normalized_altitude) * jnp.exp(-v_total)\n",
        "        theta_relative_change = jnp.abs(theta_relative) - jnp.abs(theta_relative_old)\n",
        "        shaping_reward += -1.0 * near_landing_factor * theta_relative_change\n",
        "\n",
        "        # if landing (same), reward angular_velocity -> 0\n",
        "        omega_change = jnp.abs(new_state.omega) - jnp.abs(old_state.omega)\n",
        "        shaping_reward += -0.1 * near_landing_factor * omega_change\n",
        "\n",
        "        # reward being near target (altitude -> 0, delta_angle -> 0)\n",
        "        low_altitude_factor = jnp.exp(-normalized_altitude * 2.0)\n",
        "        near_target_factor = jnp.exp(-delta_angle * 2.0)\n",
        "        landing_zone_bonus = 0.1 * low_altitude_factor * near_target_factor * near_target_factor\n",
        "        shaping_reward += landing_zone_bonus\n",
        "\n",
        "        # reward velocity -> 0 (only reward decrease, don't punish increase)\n",
        "        velocity_change = v_total - v_total_old\n",
        "        shaping_reward += -0.1 * jnp.maximum(0.0, -velocity_change)\n",
        "\n",
        "        # punish high angular velocity\n",
        "        high_omega = jnp.abs(new_state.omega) > 2.0\n",
        "        angular_velocity_punishment = 0.1 * high_omega * (jnp.abs(new_state.omega) - 2.0)\n",
        "        shaping_reward -= angular_velocity_punishment\n",
        "\n",
        "        # punish (angular) distance from target\n",
        "        distance_from_target_penalty = 0.02 * delta_angle\n",
        "        shaping_reward -= distance_from_target_penalty\n",
        "\n",
        "        # FINAL REWARD: Terminal rewards REPLACE shaping, not add\n",
        "        reward = jnp.where(is_terminal, terminal_reward, shaping_reward)\n",
        "\n",
        "        return reward\n",
        "\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        \"\"\"Environment name.\"\"\"\n",
        "        return \"RocketLander\"\n",
        "\n",
        "    @property\n",
        "    def num_actions(self) -> int:\n",
        "        \"\"\"Number of actions possible in environment.\"\"\"\n",
        "        return 2\n",
        "\n",
        "    def action_space(self, params: Optional[EnvParams] = None) -> spaces.Box:\n",
        "        \"\"\"Action space of the environment.\"\"\"\n",
        "        return spaces.Box(\n",
        "            low=-1.0,\n",
        "            high=1.0,\n",
        "            shape=(2,),\n",
        "            dtype=jnp.float32,\n",
        "        )\n",
        "\n",
        "    def observation_space(self, params: Optional[EnvParams] = None) -> spaces.Box:\n",
        "        \"\"\"Observation space of the environment.\"\"\"\n",
        "        # [altitude, delta_angle, radial_vel, tangential_vel, theta_relative, omega, throttle, gimbal, fuel]\n",
        "        low = jnp.array([-10.0, -jnp.pi, -50.0, -50.0, -jnp.pi, -10.0, 0.0, -0.5, 0.0])\n",
        "        high = jnp.array([150.0, jnp.pi, 50.0, 50.0, jnp.pi, 10.0, 1.0, 0.5, 5000.0])\n",
        "        return spaces.Box(low, high, shape=(9,), dtype=jnp.float32)\n",
        "\n",
        "\n",
        "# Create environment instance\n",
        "env = RocketLander()\n",
        "env_params = env.default_params\n",
        "print(\"Environment created successfully!\")\n",
        "print(f\"  Observation space: {env.observation_space().shape}\")\n",
        "print(f\"  Action space: {env.action_space().shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Visualization utilities for the rocket landing environment.\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "\n",
        "def visualize_trajectory(\n",
        "    states: list,\n",
        "    params: EnvParams,\n",
        "    title: str = \"Rocket Landing Trajectory\",\n",
        "    save_path: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize the rocket trajectory around the planet.\n",
        "\n",
        "    Args:\n",
        "        states: List of EnvState objects from an episode\n",
        "        params: Environment parameters\n",
        "        title: Plot title\n",
        "        save_path: If provided, save figure to this path\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "\n",
        "    # Draw planet\n",
        "    theta = np.linspace(0, 2 * np.pi, 100)\n",
        "    planet_x = params.planet_radius * np.cos(theta)\n",
        "    planet_y = params.planet_radius * np.sin(theta)\n",
        "    ax.fill(planet_x, planet_y, color='#3d5a80', alpha=0.8, label='Planet')\n",
        "    ax.plot(planet_x, planet_y, color='#1d3557', linewidth=2)\n",
        "\n",
        "    # Extract trajectory\n",
        "    xs = np.array([float(s.x) for s in states])\n",
        "    ys = np.array([float(s.y) for s in states])\n",
        "    throttles = np.array([float(s.throttle) for s in states])\n",
        "\n",
        "    # Draw trajectory colored by throttle\n",
        "    points = np.array([xs, ys]).T.reshape(-1, 1, 2)\n",
        "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "    norm = plt.Normalize(0, 1)\n",
        "    lc = LineCollection(segments, cmap='YlOrRd', norm=norm, linewidth=2, alpha=0.8)\n",
        "    lc.set_array(throttles[:-1])\n",
        "    ax.add_collection(lc)\n",
        "    cbar = plt.colorbar(lc, ax=ax, label='Throttle')\n",
        "    cbar.ax.yaxis.set_tick_params(color='white')\n",
        "    cbar.ax.yaxis.label.set_color('white')\n",
        "    plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color='white')\n",
        "\n",
        "    # Mark start and end\n",
        "    ax.scatter(xs[0], ys[0], s=100, c='green', marker='o', zorder=5, label='Start')\n",
        "    ax.scatter(xs[-1], ys[-1], s=100, c='red', marker='x', zorder=5, label='End')\n",
        "\n",
        "    # Draw target landing site\n",
        "    target_angle = float(states[0].target_angle)\n",
        "    target_x = params.planet_radius * np.cos(target_angle)\n",
        "    target_y = params.planet_radius * np.sin(target_angle)\n",
        "    ax.scatter(target_x, target_y, s=200, c='yellow', marker='*', zorder=5,\n",
        "               edgecolors='black', label='Target')\n",
        "\n",
        "    # Draw rocket orientation arrows\n",
        "    n_arrows = min(30, len(states))\n",
        "    arrow_indices = np.linspace(0, len(states) - 1, n_arrows, dtype=int)\n",
        "    for idx in arrow_indices:\n",
        "        s = states[idx]\n",
        "        x, y = float(s.x), float(s.y)\n",
        "        theta = float(s.theta)\n",
        "        dx = np.sin(theta) * 8\n",
        "        dy = np.cos(theta) * 8\n",
        "        ax.arrow(x, y, dx, dy, head_width=4, head_length=2, fc='#cccccc', ec='white', alpha=0.9)\n",
        "\n",
        "    # Set field of view\n",
        "    fov = params.init_max_orbit_radius * 1.2\n",
        "    ax.set_xlim(-fov, fov)\n",
        "    ax.set_ylim(-fov, fov)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlabel('X Position')\n",
        "    ax.set_ylabel('Y Position')\n",
        "    ax.set_title(title)\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Dark background for space\n",
        "    ax.set_facecolor('#0d1b2a')\n",
        "    fig.patch.set_facecolor('#0d1b2a')\n",
        "    ax.tick_params(colors='white')\n",
        "    ax.xaxis.label.set_color('white')\n",
        "    ax.yaxis.label.set_color('white')\n",
        "    ax.title.set_color('white')\n",
        "    ax.legend(facecolor='#1b263b', edgecolor='white', labelcolor='white')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, facecolor=fig.get_facecolor())\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "print(\"Visualization utilities loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Policy and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "from jax.random import PRNGKey\n",
        "import optax\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Neural Network\n",
        "# =============================================================================\n",
        "\n",
        "def initialize_mlp(layer_sizes, key: PRNGKey, scale: float = 1e-2):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        layer_sizes (tuple) Tuple of shapes of the neural network layers.\n",
        "        key (PRNGKey)\n",
        "        scale (float) standard deviation of initial weights and biases\n",
        "\n",
        "    Return:\n",
        "        params (List) Tuple of weights and biases\n",
        "    \"\"\"\n",
        "\n",
        "    keys = jr.split(key, 2 * len(layer_sizes))\n",
        "    params = []\n",
        "\n",
        "    for i in range(len(layer_sizes) - 1):\n",
        "        input_size, output_size = layer_sizes[i], layer_sizes[i + 1]\n",
        "        W = jr.normal(keys[2 * i], (input_size, output_size)) * scale\n",
        "        b = jr.normal(keys[2 * i + 1], (output_size,)) * scale\n",
        "        params.append((W, b))\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def policy(params, x):\n",
        "    \"\"\"\n",
        "    Gaussian policy network.\n",
        "    \n",
        "    Inputs:\n",
        "        params (list): Network parameters as list of (W, b) tuples.\n",
        "        x (array): Input observation of shape (obs_dim,).\n",
        "        \n",
        "    Returns:\n",
        "        mean (array): Mean of Gaussian for each action dimension.\n",
        "        log_std (array): Log standard deviation for each action dimension.\n",
        "    \"\"\"\n",
        "    # Shared layers\n",
        "    for (W, b) in params[:-1]:\n",
        "        x = jnp.dot(x, W) + b\n",
        "        x = jax.nn.relu(x)\n",
        "\n",
        "    # Output layer: first half is mean, second half is log_std\n",
        "    W, b = params[-1]\n",
        "    output = jnp.dot(x, W) + b\n",
        "    \n",
        "    action_dim = output.shape[-1] // 2\n",
        "    mean = jnp.tanh(output[:action_dim])  # Bound mean to [-1, 1]\n",
        "    log_std = output[action_dim:]\n",
        "    log_std = jnp.clip(log_std, -2.0, 0.5)  # Bound std to reasonable range\n",
        "    \n",
        "    return mean, log_std\n",
        "\n",
        "\n",
        "def get_action(params, x, key: PRNGKey):\n",
        "    \"\"\"\n",
        "    Sample an action from the Gaussian policy.\n",
        "\n",
        "    Inputs:\n",
        "        params (list): Network parameters.\n",
        "        x (array): Input observation.\n",
        "        key (PRNGKey): Random key for sampling.\n",
        "     \n",
        "    Returns:\n",
        "        action (array): Sampled action, clipped to [-1, 1].\n",
        "        mean (array): Mean of the Gaussian.\n",
        "        log_std (array): Log standard deviation.\n",
        "    \"\"\"\n",
        "    mean, log_std = policy(params, x)\n",
        "    std = jnp.exp(log_std)\n",
        "    noise = jr.normal(key, shape=mean.shape)\n",
        "    action = mean + std * noise\n",
        "    action = jnp.clip(action, -1.0, 1.0)  # Clip to valid action range\n",
        "    return action, mean, log_std\n",
        "\n",
        "\n",
        "def get_log_prob(params, x, action):\n",
        "    \"\"\"\n",
        "    Compute log probability of action under the Gaussian policy.\n",
        "\n",
        "    Inputs:\n",
        "        params (list): Network parameters.\n",
        "        x (array): Input observation.\n",
        "        action (array): Action taken.\n",
        "     \n",
        "    Returns:\n",
        "        log_prob (float): Log probability of the action.\n",
        "    \"\"\"\n",
        "    mean, log_std = policy(params, x)\n",
        "    std = jnp.exp(log_std)\n",
        "    \n",
        "    # Gaussian log probability\n",
        "    log_prob = -0.5 * jnp.sum(\n",
        "        ((action - mean) / std) ** 2 + 2 * log_std + jnp.log(2 * jnp.pi)\n",
        "    )\n",
        "    return log_prob\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def update_delta(delta, grad_theta):\n",
        "    \"\"\"\n",
        "    Update the parameter gradients delta with new gradient.\n",
        "    \"\"\"\n",
        "    updated_delta = jax.tree.map(lambda x, y: x + y, delta, grad_theta)\n",
        "    return updated_delta, None\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Rollout Function\n",
        "# =============================================================================\n",
        "\n",
        "def rollout(params, env_params, rng_input: PRNGKey, steps_in_episode: int):\n",
        "    \"\"\"\n",
        "    Rollout an episode using the policy.\n",
        "    \n",
        "    Inputs:\n",
        "        params: Network parameters.\n",
        "        env_params: Environment parameters.\n",
        "        rng_input: Random key.\n",
        "        steps_in_episode: Number of steps to run.\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (obs, state, action, reward, next_obs, done) arrays.\n",
        "    \"\"\"\n",
        "    rng_reset, rng_episode = jr.split(rng_input)\n",
        "    obs, state = env.reset_env(rng_reset, env_params)\n",
        "\n",
        "    def policy_step(state_input, tmp):\n",
        "        \"\"\"Single step of the policy in the environment.\"\"\"\n",
        "        obs, state, rng = state_input\n",
        "        rng, rng_action, rng_step = jr.split(rng, 3)\n",
        "        \n",
        "        action, mean, log_std = get_action(params, obs, rng_action)\n",
        "        next_obs, next_state, reward, done, _ = env.step_env(\n",
        "            rng_step, state, action, env_params\n",
        "        )\n",
        "        \n",
        "        carry = [next_obs, next_state, rng]\n",
        "        return carry, [obs, state, action, reward, next_obs, done]\n",
        "\n",
        "    # Scan over episode steps\n",
        "    _, scan_out = jax.lax.scan(\n",
        "        policy_step,\n",
        "        [obs, state, rng_episode],\n",
        "        (),\n",
        "        length=steps_in_episode,\n",
        "    )\n",
        "    \n",
        "    return scan_out\n",
        "\n",
        "\n",
        "jit_rollout = jax.jit(rollout, static_argnums=3)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# REINFORCE Loss\n",
        "# =============================================================================\n",
        "\n",
        "def loss_REINFORCE(params, obs, action, reward, baseline, gamma: float = 0.99):\n",
        "    \"\"\"\n",
        "    Compute REINFORCE loss with baseline.\n",
        "\n",
        "    Inputs:\n",
        "        params: Network parameters.\n",
        "        obs (array): Batch of observations (batch, time, obs_dim).\n",
        "        action (array): Batch of actions (batch, time, action_dim).\n",
        "        reward (array): Batch of rewards (batch, time).\n",
        "        baseline (array): Baseline values (time,).\n",
        "        gamma (float): Discount factor.\n",
        "\n",
        "    Returns:\n",
        "        delta: Accumulated gradients.\n",
        "        Gt (array): Discounted returns.\n",
        "    \"\"\"\n",
        "\n",
        "    def trajectory_gradients(reward, obs, action, baseline, delta):\n",
        "        G_init = 0.0\n",
        "\n",
        "        def step(carry, variables):\n",
        "            G, delta = carry\n",
        "            r, obs, action, b = variables\n",
        "\n",
        "            # Calculate discounted return and advantage\n",
        "            G = gamma * G + r\n",
        "            advantage = G - b\n",
        "\n",
        "            # Compute gradient of log probability\n",
        "            def neg_log_prob(params):\n",
        "                return -get_log_prob(params, obs, action)\n",
        "\n",
        "            grad_delta = jax.grad(neg_log_prob)(params)\n",
        "            grad_delta = jax.tree.map(lambda gd: gd * advantage, grad_delta)\n",
        "            delta, _ = update_delta(delta, grad_delta)\n",
        "\n",
        "            return (G, delta), G\n",
        "\n",
        "        # Iterate backwards in time\n",
        "        variables = (reward[::-1], obs[::-1], action[::-1], baseline[::-1])\n",
        "        (_, delta), Gt = jax.lax.scan(step, (G_init, delta), variables)\n",
        "        \n",
        "        return delta, Gt\n",
        "\n",
        "    # Parallelize over batch\n",
        "    parallel_trajectory_gradients = jax.vmap(\n",
        "        trajectory_gradients, in_axes=(0, 0, 0, None, None)\n",
        "    )\n",
        "    \n",
        "    # Initialize delta to zeros\n",
        "    delta = jax.tree.map(lambda t: jnp.zeros(t.shape), params)\n",
        "\n",
        "    # Compute gradients in parallel and sum\n",
        "    deltas, Gs = parallel_trajectory_gradients(reward, obs, action, baseline, delta)\n",
        "    delta, _ = jax.lax.scan(update_delta, delta, deltas)\n",
        "\n",
        "    return delta, jnp.array(Gs)\n",
        "\n",
        "\n",
        "loss_REINFORCE = jax.jit(loss_REINFORCE)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Baseline Functions\n",
        "# =============================================================================\n",
        "\n",
        "def mean_baseline(Gs):\n",
        "    \"\"\"Compute constant baseline as mean of discounted returns.\"\"\"\n",
        "    T = Gs.shape[1]\n",
        "    mean_reward = jnp.mean(Gs)\n",
        "    return jnp.ones((T,)) * mean_reward\n",
        "\n",
        "\n",
        "def timedependent_baseline(Gs):\n",
        "    \"\"\"Compute time-dependent baseline.\"\"\"\n",
        "    mean_reward = jnp.mean(Gs, axis=0)  # Mean over batches\n",
        "    cumulative_rewards = jnp.cumsum(mean_reward)\n",
        "    baseline = cumulative_rewards / jnp.arange(1, Gs.shape[1] + 1)\n",
        "    return mean_reward - baseline\n",
        "\n",
        "\n",
        "print(\"Training utilities loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_policy(params, key, title=\"Policy Trajectory\", save_path=None):\n",
        "    \"\"\"Run a single episode and visualize the trajectory.\"\"\"\n",
        "    # Run rollout\n",
        "    obs, state, action, reward, next_obs, done = rollout(\n",
        "        params, env_params, key, env_params.max_steps_in_episode\n",
        "    )\n",
        "    \n",
        "    # Convert states to list for visualization\n",
        "    states = []\n",
        "    for i in range(len(state.x)):\n",
        "        s = EnvState(\n",
        "            x=state.x[i],\n",
        "            y=state.y[i],\n",
        "            theta=state.theta[i],\n",
        "            dx=state.dx[i],\n",
        "            dy=state.dy[i],\n",
        "            omega=state.omega[i],\n",
        "            throttle=state.throttle[i],\n",
        "            gimbal=state.gimbal[i],\n",
        "            fuel=state.fuel[i],\n",
        "            target_angle=state.target_angle[i],\n",
        "            time=state.time[i],\n",
        "        )\n",
        "        states.append(s)\n",
        "        \n",
        "        # Stop if done\n",
        "        if done[i]:\n",
        "            break\n",
        "    \n",
        "    # Only sum rewards up to the terminal step (after that, rewards are garbage)\n",
        "    episode_length = len(states)\n",
        "    total_reward = float(jnp.sum(reward[:episode_length]))\n",
        "    print(f\"  Total reward: {total_reward:.2f}\")\n",
        "    print(f\"  Episode length: {episode_length} steps\")\n",
        "    \n",
        "    visualize_trajectory(states, env_params, title=f\"{title} (R={total_reward:.1f})\", save_path=save_path)\n",
        "\n",
        "\n",
        "def train(\n",
        "    num_iters: int = 5000,\n",
        "    steps_in_episode: int = 2000,\n",
        "    lr: float = 0.001,\n",
        "    gamma: float = 0.99,\n",
        "    n_batches: int = 32,\n",
        "    hidden_size: int = 128,\n",
        "    n_hidden_layers: int = 2,\n",
        "    visualize_every: int = 500,\n",
        "    seed: int = 42,\n",
        "    grad_clip: float = 1.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train the rocket landing policy using REINFORCE.\n",
        "    \n",
        "    Args:\n",
        "        num_iters: Number of training iterations.\n",
        "        steps_in_episode: Steps per episode.\n",
        "        lr: Learning rate.\n",
        "        gamma: Discount factor.\n",
        "        n_batches: Number of parallel episodes per iteration.\n",
        "        hidden_size: Size of hidden layers.\n",
        "        n_hidden_layers: Number of hidden layers.\n",
        "        visualize_every: Visualize trajectory every n iterations.\n",
        "        seed: Random seed.\n",
        "        grad_clip: Maximum gradient norm for clipping.\n",
        "    \"\"\"\n",
        "    key = PRNGKey(seed)\n",
        "    \n",
        "    # Network architecture: obs_dim -> hidden layers -> 2*action_dim (mean + log_std)\n",
        "    obs_dim = 9  # From rocket_env observation space\n",
        "    action_dim = 2  # throttle and gimbal\n",
        "    hidden_layers = tuple([hidden_size] * n_hidden_layers)\n",
        "    layer_sizes = (obs_dim,) + hidden_layers + (2 * action_dim,)\n",
        "    \n",
        "    # Initialize network and optimizer\n",
        "    key, subkey = jr.split(key)\n",
        "    params = initialize_mlp(layer_sizes, key=subkey)\n",
        "    \n",
        "    # Adam optimizer with gradient clipping for stability\n",
        "    optim = optax.chain(\n",
        "        optax.clip_by_global_norm(grad_clip),\n",
        "        optax.adam(learning_rate=lr),\n",
        "    )\n",
        "    opt_state = optim.init(params)\n",
        "    \n",
        "    # Initialize baseline storage\n",
        "    Gs = jnp.zeros((n_batches, steps_in_episode))\n",
        "    \n",
        "    # Training keys\n",
        "    key, subkey = jr.split(key)\n",
        "    iter_keys = jr.split(subkey, num_iters)\n",
        "    \n",
        "    # Parallel rollout function\n",
        "    parallel_rollout = jax.vmap(rollout, in_axes=(None, None, 0, None))\n",
        "    \n",
        "    # Training step function\n",
        "    def step(carry, key):\n",
        "        params, opt_state, Gs = carry\n",
        "        \n",
        "        # Generate rollouts\n",
        "        keys = jr.split(key, n_batches)\n",
        "        obs, state, action, reward, next_obs, done = parallel_rollout(\n",
        "            params, env_params, keys, steps_in_episode\n",
        "        )\n",
        "        \n",
        "        # Compute baseline from previous returns\n",
        "        baseline = mean_baseline(Gs)\n",
        "        \n",
        "        # Compute gradients\n",
        "        delta, Gs_new = loss_REINFORCE(params, obs, action, reward, baseline, gamma)\n",
        "        \n",
        "        # Update parameters\n",
        "        updates, opt_state = optim.update(delta, opt_state, params)\n",
        "        new_params = optax.apply_updates(params, updates)\n",
        "        \n",
        "        # Mean episode reward for logging\n",
        "        mean_reward = jnp.mean(jnp.sum(reward, axis=-1))\n",
        "        \n",
        "        carry = (new_params, opt_state, Gs_new)\n",
        "        return carry, mean_reward\n",
        "\n",
        "    # Compile the step function\n",
        "    step = jax.jit(step)\n",
        "    \n",
        "    # Training loop with periodic visualization\n",
        "    history = []\n",
        "    current_params = params\n",
        "    current_opt_state = opt_state\n",
        "    current_Gs = Gs\n",
        "    \n",
        "    print(f\"Starting training for {num_iters} iterations...\")\n",
        "    print(f\"  Network architecture: {layer_sizes}\")\n",
        "    print(f\"  Episodes per iteration: {n_batches}\")\n",
        "    print(f\"  Steps per episode: {steps_in_episode}\")\n",
        "    print(f\"  Learning rate: {lr}\")\n",
        "    print(f\"  Discount factor: {gamma}\")\n",
        "    print(f\"  Gradient clipping: {grad_clip}\")\n",
        "    print()\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "        # Training step\n",
        "        (current_params, current_opt_state, current_Gs), mean_reward = step(\n",
        "            (current_params, current_opt_state, current_Gs), iter_keys[i]\n",
        "        )\n",
        "        history.append(float(mean_reward))\n",
        "        \n",
        "        # Logging\n",
        "        if (i + 1) % 100 == 0:\n",
        "            avg_reward = jnp.mean(jnp.array(history[-100:]))\n",
        "            print(f\"Iteration {i + 1}/{num_iters} | Avg Reward (last 100): {avg_reward:.2f}\")\n",
        "        \n",
        "        # Visualization\n",
        "        if (i + 1) % visualize_every == 0:\n",
        "            print(f\"\\nVisualizing policy at iteration {i + 1}...\")\n",
        "            key, vis_key = jr.split(key)\n",
        "            visualize_policy(current_params, vis_key, title=f\"Iteration {i + 1}\")\n",
        "    \n",
        "    # Final visualization\n",
        "    print(f\"\\nTraining complete! Final policy visualization...\")\n",
        "    key, vis_key = jr.split(key)\n",
        "    visualize_policy(current_params, vis_key, title=\"Final Policy\")\n",
        "    \n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history, alpha=0.3, label='Episode Reward')\n",
        "    # Smoothed curve\n",
        "    window = min(100, len(history) // 10)\n",
        "    if window > 1:\n",
        "        smoothed = jnp.convolve(jnp.array(history), jnp.ones(window) / window, mode='valid')\n",
        "        plt.plot(range(window - 1, len(history)), smoothed, label=f'Smoothed (window={window})')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Mean Episode Reward')\n",
        "    plt.title('Training Progress')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return current_params, history\n",
        "\n",
        "\n",
        "print(\"Training function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Training\n",
        "\n",
        "Adjust hyperparameters as needed. The default values are optimized for GPU training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the agent\n",
        "trained_params, history = train(\n",
        "    num_iters=50000,\n",
        "    steps_in_episode=1500,\n",
        "    lr=0.0002,\n",
        "    gamma=0.995,\n",
        "    n_batches=128,\n",
        "    hidden_size=256,\n",
        "    n_hidden_layers=3,\n",
        "    visualize_every=500,\n",
        "    seed=42,\n",
        "    grad_clip=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Trained Policy\n",
        "\n",
        "Run multiple test episodes to see how the trained agent performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained policy with multiple episodes\n",
        "key = PRNGKey(123)\n",
        "for i in range(5):\n",
        "    key, subkey = jr.split(key)\n",
        "    print(f\"\\n=== Test Episode {i+1} ===\")\n",
        "    visualize_policy(trained_params, subkey, title=f\"Test Episode {i+1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save/Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save trained parameters\n",
        "with open('trained_rocket_lander.pkl', 'wb') as f:\n",
        "    pickle.dump(trained_params, f)\n",
        "print(\"Model saved to 'trained_rocket_lander.pkl'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained parameters\n",
        "with open('trained_rocket_lander.pkl', 'rb') as f:\n",
        "    loaded_params = pickle.load(f)\n",
        "print(\"Model loaded from 'trained_rocket_lander.pkl'\")\n",
        "\n",
        "# Test loaded model\n",
        "key = PRNGKey(456)\n",
        "visualize_policy(loaded_params, key, title=\"Loaded Model Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Rocket Landing with REINFORCE\n",
        "\n",
        "This notebook trains a neural network policy to land a rocket on a planet using the REINFORCE algorithm with a Gaussian policy.\n",
        "\n",
        "**Features:**\n",
        "- JAX-based physics simulation\n",
        "- Vectorized environment for fast training\n",
        "- Gaussian policy for continuous control\n",
        "- Reward shaping to guide learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q jax[cuda12] flax optax gymnax matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pickle\n",
        "from typing import Any, Optional, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "from jax.random import PRNGKey\n",
        "import optax\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.collections import LineCollection\n",
        "from flax import struct\n",
        "from gymnax.environments import environment, spaces\n",
        "\n",
        "# Verify JAX is using GPU\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"JAX backend: {jax.default_backend()}\")\n",
        "print(f\"JAX devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Environment Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gravitational constant\n",
        "G = 6.67430e-11\n",
        "\n",
        "\n",
        "def angle_normalize(x: jax.Array) -> jax.Array:\n",
        "    \"\"\"Normalize the angle - radians.\"\"\"\n",
        "    return ((x + jnp.pi) % (2 * jnp.pi)) - jnp.pi\n",
        "\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvState(environment.EnvState):\n",
        "    \"\"\"State of the rocket landing environment.\"\"\"\n",
        "    # position\n",
        "    x: jnp.ndarray\n",
        "    y: jnp.ndarray\n",
        "    theta: jnp.ndarray\n",
        "\n",
        "    # velocity\n",
        "    dx: jnp.ndarray\n",
        "    dy: jnp.ndarray\n",
        "    omega: jnp.ndarray\n",
        "\n",
        "    # engine\n",
        "    throttle: jnp.ndarray\n",
        "    gimbal: jnp.ndarray\n",
        "\n",
        "    # fuel\n",
        "    fuel: jnp.ndarray\n",
        "\n",
        "    # target landing angle\n",
        "    target_angle: jnp.ndarray\n",
        "\n",
        "    # timestep\n",
        "    time: jnp.ndarray\n",
        "\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvParams(environment.EnvParams):\n",
        "    \"\"\"Parameters for the rocket landing environment.\"\"\"\n",
        "    max_steps_in_episode: int = 1500\n",
        "    dt: float = 0.05\n",
        "\n",
        "    # planet properties\n",
        "    planet_radius: float = 50.0\n",
        "    planet_mass: float = 5.0e14\n",
        "\n",
        "    # rocket properties\n",
        "    rocket_max_thrust: float = 38000.0\n",
        "    rocket_max_gimbal: float = 0.3\n",
        "    rocket_mass: float = 2600.0\n",
        "    rocket_moment_of_inertia: float = 250.0\n",
        "    rocket_initial_fuel: float = 5000.0\n",
        "    rocket_fuel_consumption_rate: float = 100.\n",
        "\n",
        "    # initialization parameters\n",
        "    init_min_orbit_radius: float = 80.0\n",
        "    init_max_orbit_radius: float = 150.0\n",
        "    init_orbit_velocity_noise: float = 0.1\n",
        "\n",
        "    # good landing thresholds\n",
        "    landing_max_speed: float = 2.0\n",
        "    landing_max_angle: float = 0.3\n",
        "    landing_max_omega: float = 0.5\n",
        "    landing_position_tolerance: float = jnp.pi/30\n",
        "\n",
        "    # terminal rewards\n",
        "    reward_landed: float = 100.0\n",
        "    reward_sl_cp_co_ls: float = 1000.0\n",
        "    reward_nsl_cp: float = 200.0\n",
        "    reward_nsl_ncp: float = 100.0\n",
        "    reward_sl_ncp_co_ls: float = 500.0\n",
        "    reward_out_of_fuel: float = -50.0\n",
        "    reward_out_of_bounds: float = -1000.0\n",
        "    reward_timeout: float = -2000.0\n",
        "\n",
        "    # non-terminal rewards\n",
        "    reward_retrograde_burn: float = 0.02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RocketLander(environment.Environment[EnvState, EnvParams]):\n",
        "    \"\"\"JAX/Gymnax implementation of a 2D rocket landing environment.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @property\n",
        "    def default_params(self) -> EnvParams:\n",
        "        return EnvParams()\n",
        "\n",
        "    def step_env(self, key: jax.Array, state: EnvState, action: jax.Array, params: EnvParams):\n",
        "        \"\"\"Integrate physics for one timestep.\"\"\"\n",
        "        # Gravity\n",
        "        r = jnp.sqrt(state.x**2 + state.y**2)\n",
        "        a_gravity_magnitude = G * params.planet_mass / r**2\n",
        "        a_g_x = -a_gravity_magnitude * state.x / r\n",
        "        a_g_y = -a_gravity_magnitude * state.y / r\n",
        "\n",
        "        # Throttle and gimbal control\n",
        "        throttle = jnp.clip(action[0], -1.0, 1.0)\n",
        "        gimbal = jnp.clip(action[1], -1.0, 1.0)\n",
        "        throttle = (throttle + 1.0)/2.0\n",
        "        gimbal *= params.rocket_max_gimbal\n",
        "        throttle = jnp.minimum(throttle, state.fuel / (params.rocket_fuel_consumption_rate * params.dt))\n",
        "\n",
        "        # Thrust\n",
        "        thrust_angle = state.theta + gimbal\n",
        "        F_thrust = throttle * params.rocket_max_thrust\n",
        "        a_t_x = F_thrust * jnp.sin(thrust_angle) / params.rocket_mass\n",
        "        a_t_y = F_thrust * jnp.cos(thrust_angle) / params.rocket_mass\n",
        "        F_torque = F_thrust * jnp.sin(gimbal)\n",
        "        a_omega = F_torque / params.rocket_moment_of_inertia\n",
        "\n",
        "        # Physics integration\n",
        "        a_x, a_y = a_g_x + a_t_x, a_g_y + a_t_y\n",
        "        dx = state.dx + a_x * params.dt\n",
        "        dy = state.dy + a_y * params.dt\n",
        "        omega = state.omega + a_omega * params.dt\n",
        "        x = state.x + dx * params.dt\n",
        "        y = state.y + dy * params.dt\n",
        "        theta = state.theta + omega * params.dt\n",
        "        fuel = jnp.maximum(0.0, state.fuel - throttle * params.rocket_fuel_consumption_rate * params.dt)\n",
        "\n",
        "        new_state = EnvState(x=x, y=y, theta=theta, dx=dx, dy=dy, omega=omega,\n",
        "                             throttle=throttle, gimbal=gimbal, fuel=fuel,\n",
        "                             target_angle=state.target_angle, time=state.time+params.dt)\n",
        "        \n",
        "        done = self.is_terminal(new_state, params)\n",
        "        reward = self._compute_reward(state, new_state, params)\n",
        "        key, noise_key = jax.random.split(key)\n",
        "        obs = self.get_obs(new_state, params, noise_key)\n",
        "        return obs, new_state, reward, done, {}\n",
        "\n",
        "    def reset_env(self, key: jax.Array, params: EnvParams):\n",
        "        \"\"\"Reset environment by sampling initial orbit.\"\"\"\n",
        "        key, angle_key, orbit_radius_key, orbit_vel_noise_key, target_angle_key = jax.random.split(key, 5)\n",
        "        \n",
        "        angle_init = jax.random.uniform(angle_key, minval=0.0, maxval=2.0 * jnp.pi)\n",
        "        orbit_radius_init = jax.random.uniform(orbit_radius_key, minval=params.init_min_orbit_radius, maxval=params.init_max_orbit_radius)\n",
        "        \n",
        "        x_init = orbit_radius_init * jnp.cos(angle_init)\n",
        "        y_init = orbit_radius_init * jnp.sin(angle_init)\n",
        "        \n",
        "        orbit_velocity_magnitude_init = jnp.sqrt(G * params.planet_mass / orbit_radius_init)\n",
        "        velocity_noise = params.init_orbit_velocity_noise * (2.0 * jax.random.uniform(orbit_vel_noise_key) - 1.0)\n",
        "        orbit_velocity_magnitude_init *= (1.0 + velocity_noise)\n",
        "        \n",
        "        dx_init = -orbit_velocity_magnitude_init * jnp.sin(angle_init)\n",
        "        dy_init = orbit_velocity_magnitude_init * jnp.cos(angle_init)\n",
        "        theta_init = angle_init + (3/2)*jnp.pi\n",
        "        target_angle = jax.random.uniform(target_angle_key, minval=0.0, maxval=2.0 * jnp.pi)\n",
        "\n",
        "        state = EnvState(x=x_init, y=y_init, theta=theta_init, dx=dx_init, dy=dy_init,\n",
        "                         omega=jnp.array(0.0), throttle=jnp.array(0.0), gimbal=jnp.array(0.0),\n",
        "                         fuel=jnp.array(params.rocket_initial_fuel), target_angle=target_angle, time=jnp.array(0))\n",
        "        return self.get_obs(state, params, key), state\n",
        "\n",
        "    def get_obs(self, state: EnvState, params: EnvParams=None, key: jax.Array=None) -> jax.Array:\n",
        "        \"\"\"Get observation from state.\"\"\"\n",
        "        if params is None:\n",
        "            params = self.default_params\n",
        "        r = jnp.sqrt(state.x ** 2 + state.y ** 2)\n",
        "        altitude = r - params.planet_radius\n",
        "        positional_angle = jnp.arctan2(state.y, state.x)\n",
        "        delta_angle = angle_normalize(state.target_angle - positional_angle)\n",
        "        radial_vel = (state.x * state.dx + state.y * state.dy) / r\n",
        "        tangential_vel = (state.x * state.dy - state.y * state.dx) / r\n",
        "        theta_relative = angle_normalize(state.theta - positional_angle)\n",
        "        return jnp.array([altitude, delta_angle, radial_vel, tangential_vel,\n",
        "                          theta_relative, state.omega, state.throttle, state.gimbal, state.fuel])\n",
        "\n",
        "    def is_terminal(self, state: EnvState, params: EnvParams) -> jax.Array:\n",
        "        \"\"\"Check whether state is terminal.\"\"\"\n",
        "        max_time = params.max_steps_in_episode * params.dt\n",
        "        timeout = state.time >= max_time\n",
        "        r = jnp.sqrt(state.x ** 2 + state.y ** 2)\n",
        "        landed = r <= params.planet_radius\n",
        "        escaped = r > params.planet_radius * 4\n",
        "        return jnp.array(landed | timeout | escaped)\n",
        "\n",
        "    def _compute_reward(self, old_state: EnvState, new_state: EnvState, params: EnvParams) -> jax.Array:\n",
        "        \"\"\"Compute reward with shaping.\"\"\"\n",
        "        # New state calculations\n",
        "        r = jnp.sqrt(new_state.x ** 2 + new_state.y ** 2)\n",
        "        altitude = r - params.planet_radius\n",
        "        normalized_altitude = jnp.clip(altitude / (params.init_max_orbit_radius - params.planet_radius), 0.0, 2.0)\n",
        "\n",
        "        landed = r <= params.planet_radius\n",
        "        escaped = r > params.planet_radius * 4\n",
        "        max_time = params.max_steps_in_episode * params.dt\n",
        "        timeout = new_state.time >= max_time\n",
        "        is_terminal = landed | escaped | timeout\n",
        "\n",
        "        positional_angle = jnp.arctan2(new_state.y, new_state.x)\n",
        "        delta_angle = jnp.abs(angle_normalize(new_state.target_angle - positional_angle))\n",
        "        v_total = jnp.sqrt(new_state.dx ** 2 + new_state.dy ** 2)\n",
        "        v_tangential = (new_state.x * new_state.dy - new_state.y * new_state.dx) / r\n",
        "        theta_relative = angle_normalize(new_state.theta - positional_angle)\n",
        "\n",
        "        # Old state calculations\n",
        "        r_old = jnp.sqrt(old_state.x ** 2 + old_state.y ** 2)\n",
        "        normalized_altitude_old = jnp.clip((r_old - params.planet_radius) / (params.init_max_orbit_radius - params.planet_radius), 0.0, 2.0)\n",
        "        positional_angle_old = jnp.arctan2(old_state.y, old_state.x)\n",
        "        v_total_old = jnp.sqrt(old_state.dx ** 2 + old_state.dy ** 2)\n",
        "        v_tangential_old = (old_state.x * old_state.dy - old_state.y * old_state.dx) / r_old\n",
        "        theta_relative_old = jnp.abs(angle_normalize(old_state.theta - positional_angle_old))\n",
        "\n",
        "        # Landing conditions\n",
        "        slow = v_total < params.landing_max_speed\n",
        "        correct_position = delta_angle <= params.landing_position_tolerance\n",
        "        correct_orientation = jnp.abs(theta_relative) <= params.landing_max_angle\n",
        "        low_spin = jnp.abs(new_state.omega) <= params.landing_max_omega\n",
        "\n",
        "        # Terminal rewards\n",
        "        terminal_reward = jnp.array(0.0)\n",
        "        sl_cp_co_ls = landed & slow & correct_position & correct_orientation & low_spin\n",
        "        terminal_reward = jnp.where(sl_cp_co_ls, params.reward_sl_cp_co_ls, terminal_reward)\n",
        "        sl_ncp_co_ls = landed & slow & ~correct_position & correct_orientation & low_spin\n",
        "        terminal_reward = jnp.where(sl_ncp_co_ls, params.reward_sl_ncp_co_ls * jnp.exp(-delta_angle * 2.0), terminal_reward)\n",
        "        sl_nco_nls = landed & slow & ~(correct_orientation & low_spin)\n",
        "        terminal_reward = jnp.where(sl_nco_nls, 100.0 * jnp.exp(-delta_angle), terminal_reward)\n",
        "        nsl_cp = landed & ~slow & correct_position\n",
        "        terminal_reward = jnp.where(nsl_cp, params.reward_nsl_cp, terminal_reward)\n",
        "        nsl_ncp = landed & ~slow & ~correct_position\n",
        "        terminal_reward = jnp.where(nsl_ncp, params.reward_nsl_ncp, terminal_reward)\n",
        "        terminal_reward = jnp.where(escaped, params.reward_out_of_bounds, terminal_reward)\n",
        "        terminal_reward = jnp.where(timeout & ~landed & ~escaped, params.reward_timeout, terminal_reward)\n",
        "\n",
        "        ### Non-terminal (shaping) rewards\n",
        "        shaping_reward = jnp.array(0.0)\n",
        "\n",
        "        # Reward for firing engine while pointed retrograde (opposite to velocity direction)\n",
        "        # This encourages braking burns to slow down\n",
        "        thrust_angle = new_state.theta + new_state.gimbal\n",
        "        velocity_angle = jnp.arctan2(new_state.dy, new_state.dx)\n",
        "        alignment = jnp.cos(thrust_angle - velocity_angle)  # 1 = prograde, -1 = retrograde\n",
        "        retrograde_alignment = (1.0 - alignment) / 2.0  # 0 = prograde, 1 = retrograde\n",
        "        throttle_used = new_state.throttle > 0.1\n",
        "        velocity_factor = jnp.minimum(v_total / 10.0, 1.0)\n",
        "        retrograde_burn_reward = params.reward_retrograde_burn * retrograde_alignment * throttle_used * velocity_factor\n",
        "        shaping_reward += retrograde_burn_reward\n",
        "\n",
        "        # Reward for decreasing tangential velocity (we want v_tangential -> 0)\n",
        "        # Negative coefficient because we want to reward when (new - old) is negative\n",
        "        tangential_velocity_change = jnp.abs(v_tangential) - jnp.abs(v_tangential_old)\n",
        "        shaping_reward += -0.3 * tangential_velocity_change\n",
        "\n",
        "        # Reward for decreasing altitude, but only when near the target landing spot\n",
        "        # Weight by exp(-delta_angle) so we don't reward descending on the wrong side\n",
        "        delta_angle_factor = jnp.exp(-delta_angle * 3)\n",
        "        altitude_change = normalized_altitude - normalized_altitude_old\n",
        "        shaping_reward += -1.0 * delta_angle_factor * altitude_change\n",
        "\n",
        "        # Reward for improving orientation (theta_relative -> 0), but only when near landing\n",
        "        # near_landing_factor is high only when both altitude and velocity are low\n",
        "        near_landing_factor = jnp.exp(-normalized_altitude * 3.0) * jnp.exp(-v_total)\n",
        "        theta_relative_change = jnp.abs(theta_relative) - jnp.abs(theta_relative_old)\n",
        "        shaping_reward += -1.0 * near_landing_factor * theta_relative_change\n",
        "\n",
        "        # Reward for decreasing angular velocity (omega -> 0), but only when near landing\n",
        "        omega_change = jnp.abs(new_state.omega) - jnp.abs(old_state.omega)\n",
        "        shaping_reward += -0.1 * near_landing_factor * omega_change\n",
        "\n",
        "        # Bonus for being in the \"landing zone\" (low altitude AND near target)\n",
        "        # This is a constant bonus per timestep, not a change-based reward\n",
        "        low_altitude_factor = jnp.exp(-normalized_altitude * 2.0)\n",
        "        near_target_factor = jnp.exp(-delta_angle * 2.0)\n",
        "        landing_zone_bonus = 0.1 * low_altitude_factor * near_target_factor * near_target_factor\n",
        "        shaping_reward += landing_zone_bonus\n",
        "\n",
        "        # Reward for decreasing total velocity (we want v_total -> 0)\n",
        "        # Only reward decreases, don't punish increases (use max with 0)\n",
        "        velocity_change = v_total - v_total_old\n",
        "        shaping_reward += -0.1 * jnp.maximum(0.0, -velocity_change)  # reward when velocity_change < 0\n",
        "\n",
        "        # Punish high angular velocity (spinning too fast)\n",
        "        high_omega = jnp.abs(new_state.omega) > 2.0\n",
        "        angular_velocity_punishment = 0.1 * high_omega * (jnp.abs(new_state.omega) - 2.0)\n",
        "        shaping_reward -= angular_velocity_punishment\n",
        "\n",
        "        # Constant penalty for being far from target (angular distance)\n",
        "        # This discourages taking the \"long way around\" the planet\n",
        "        distance_from_target_penalty = 0.02 * delta_angle\n",
        "        shaping_reward -= distance_from_target_penalty\n",
        "\n",
        "        return jnp.where(is_terminal, terminal_reward, shaping_reward)\n",
        "\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return \"RocketLander\"\n",
        "\n",
        "    @property\n",
        "    def num_actions(self) -> int:\n",
        "        return 2\n",
        "\n",
        "    def action_space(self, params: Optional[EnvParams] = None) -> spaces.Box:\n",
        "        return spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=jnp.float32)\n",
        "\n",
        "    def observation_space(self, params: Optional[EnvParams] = None) -> spaces.Box:\n",
        "        low = jnp.array([-10.0, -jnp.pi, -50.0, -50.0, -jnp.pi, -10.0, 0.0, -0.5, 0.0])\n",
        "        high = jnp.array([150.0, jnp.pi, 50.0, 50.0, jnp.pi, 10.0, 1.0, 0.5, 5000.0])\n",
        "        return spaces.Box(low, high, shape=(9,), dtype=jnp.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualization Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_trajectory(states: list, params: EnvParams, title: str = \"Rocket Landing Trajectory\"):\n",
        "    \"\"\"Visualize the rocket trajectory around the planet.\"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "\n",
        "    # Draw planet\n",
        "    theta = np.linspace(0, 2 * np.pi, 100)\n",
        "    planet_x = params.planet_radius * np.cos(theta)\n",
        "    planet_y = params.planet_radius * np.sin(theta)\n",
        "    ax.fill(planet_x, planet_y, color='#3d5a80', alpha=0.8, label='Planet')\n",
        "    ax.plot(planet_x, planet_y, color='#1d3557', linewidth=2)\n",
        "\n",
        "    # Extract trajectory\n",
        "    xs = np.array([float(s.x) for s in states])\n",
        "    ys = np.array([float(s.y) for s in states])\n",
        "    throttles = np.array([float(s.throttle) for s in states])\n",
        "\n",
        "    # Draw trajectory colored by throttle\n",
        "    points = np.array([xs, ys]).T.reshape(-1, 1, 2)\n",
        "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "    norm = plt.Normalize(0, 1)\n",
        "    lc = LineCollection(segments, cmap='YlOrRd', norm=norm, linewidth=2, alpha=0.8)\n",
        "    lc.set_array(throttles[:-1])\n",
        "    ax.add_collection(lc)\n",
        "    cbar = plt.colorbar(lc, ax=ax, label='Throttle')\n",
        "    cbar.ax.yaxis.set_tick_params(color='white')\n",
        "    cbar.ax.yaxis.label.set_color('white')\n",
        "    plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color='white')\n",
        "\n",
        "    # Mark start and end\n",
        "    ax.scatter(xs[0], ys[0], s=100, c='green', marker='o', zorder=5, label='Start')\n",
        "    ax.scatter(xs[-1], ys[-1], s=100, c='red', marker='x', zorder=5, label='End')\n",
        "\n",
        "    # Draw target landing site\n",
        "    target_angle = float(states[0].target_angle)\n",
        "    target_x = params.planet_radius * np.cos(target_angle)\n",
        "    target_y = params.planet_radius * np.sin(target_angle)\n",
        "    ax.scatter(target_x, target_y, s=200, c='yellow', marker='*', zorder=5, edgecolors='black', label='Target')\n",
        "\n",
        "    # Draw rocket orientation arrows\n",
        "    n_arrows = min(30, len(states))\n",
        "    arrow_indices = np.linspace(0, len(states) - 1, n_arrows, dtype=int)\n",
        "    for idx in arrow_indices:\n",
        "        s = states[idx]\n",
        "        x, y = float(s.x), float(s.y)\n",
        "        th = float(s.theta)\n",
        "        dx = np.sin(th) * 8\n",
        "        dy = np.cos(th) * 8\n",
        "        ax.arrow(x, y, dx, dy, head_width=4, head_length=2, fc='#cccccc', ec='white', alpha=0.9)\n",
        "\n",
        "    # Set view\n",
        "    fov = params.init_max_orbit_radius * 1.2\n",
        "    ax.set_xlim(-fov, fov)\n",
        "    ax.set_ylim(-fov, fov)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlabel('X Position')\n",
        "    ax.set_ylabel('Y Position')\n",
        "    ax.set_title(title)\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Dark background\n",
        "    ax.set_facecolor('#0d1b2a')\n",
        "    fig.patch.set_facecolor('#0d1b2a')\n",
        "    ax.tick_params(colors='white')\n",
        "    ax.xaxis.label.set_color('white')\n",
        "    ax.yaxis.label.set_color('white')\n",
        "    ax.title.set_color('white')\n",
        "    ax.legend(facecolor='#1b263b', edgecolor='white', labelcolor='white')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Neural Network Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_mlp(layer_sizes, key: PRNGKey, scale: float = 1e-2):\n",
        "    \"\"\"Initialize MLP parameters.\"\"\"\n",
        "    keys = jr.split(key, 2 * len(layer_sizes))\n",
        "    params = []\n",
        "    for i in range(len(layer_sizes) - 1):\n",
        "        input_size, output_size = layer_sizes[i], layer_sizes[i + 1]\n",
        "        W = jr.normal(keys[2 * i], (input_size, output_size)) * scale\n",
        "        b = jr.normal(keys[2 * i + 1], (output_size,)) * scale\n",
        "        params.append((W, b))\n",
        "    return params\n",
        "\n",
        "\n",
        "def policy(params, x):\n",
        "    \"\"\"Gaussian policy network.\"\"\"\n",
        "    for (W, b) in params[:-1]:\n",
        "        x = jnp.dot(x, W) + b\n",
        "        x = jax.nn.relu(x)\n",
        "\n",
        "    W, b = params[-1]\n",
        "    output = jnp.dot(x, W) + b\n",
        "    \n",
        "    action_dim = output.shape[-1] // 2\n",
        "    mean = jnp.tanh(output[:action_dim])\n",
        "    log_std = jnp.clip(output[action_dim:], -2.0, 0.5)\n",
        "    \n",
        "    return mean, log_std\n",
        "\n",
        "\n",
        "def get_action(params, x, key: PRNGKey):\n",
        "    \"\"\"Sample an action from the Gaussian policy.\"\"\"\n",
        "    mean, log_std = policy(params, x)\n",
        "    std = jnp.exp(log_std)\n",
        "    noise = jr.normal(key, shape=mean.shape)\n",
        "    action = jnp.clip(mean + std * noise, -1.0, 1.0)\n",
        "    return action, mean, log_std\n",
        "\n",
        "\n",
        "def get_log_prob(params, x, action):\n",
        "    \"\"\"Compute log probability of action under the Gaussian policy.\"\"\"\n",
        "    mean, log_std = policy(params, x)\n",
        "    std = jnp.exp(log_std)\n",
        "    log_prob = -0.5 * jnp.sum(((action - mean) / std) ** 2 + 2 * log_std + jnp.log(2 * jnp.pi))\n",
        "    return log_prob\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def update_delta(delta, grad_theta):\n",
        "    \"\"\"Update the parameter gradients.\"\"\"\n",
        "    return jax.tree.map(lambda x, y: x + y, delta, grad_theta), None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Rocket Landing with Reinforcement Learning\n",
        "\n",
        "This notebook trains a neural network policy to land a rocket on a planet using REINFORCE.\n",
        "\n",
        "**Features:**\n",
        "- 2D physics simulation with gravity\n",
        "- Rocket starts in orbit, must land at target location\n",
        "- Continuous control: throttle and gimbal angle\n",
        "- GPU-accelerated training with JAX\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's check if we have GPU access and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q jax[cuda12] flax optax gymnax matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify JAX GPU setup\n",
        "import jax\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Backend: {jax.default_backend()}\")\n",
        "print(f\"Devices: {jax.devices()}\")\n",
        "\n",
        "if jax.default_backend() == 'gpu':\n",
        "    print(\"\\nâœ… GPU acceleration enabled!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Running on CPU. Go to Runtime > Change runtime type > GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Definition\n",
        "\n",
        "The rocket environment simulates:\n",
        "- A planet with gravitational pull\n",
        "- A rocket that starts in orbit\n",
        "- Target landing location on the planet surface\n",
        "- Fuel consumption and thrust physics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"JAX implementation of rocket landing environment.\"\"\"\n",
        "from typing import Any, Optional, Tuple\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import struct\n",
        "from gymnax.environments import environment, spaces\n",
        "\n",
        "# GRAVITATIONAL_CONSTANT\n",
        "G = 6.67430e-11\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvState(environment.EnvState):\n",
        "    \"\"\"State of the rocket landing environment.\"\"\"\n",
        "    x: jnp.ndarray\n",
        "    y: jnp.ndarray\n",
        "    theta: jnp.ndarray\n",
        "    dx: jnp.ndarray\n",
        "    dy: jnp.ndarray\n",
        "    omega: jnp.ndarray\n",
        "    throttle: jnp.ndarray\n",
        "    gimbal: jnp.ndarray\n",
        "    fuel: jnp.ndarray\n",
        "    target_angle: jnp.ndarray\n",
        "    time: jnp.ndarray\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvParams(environment.EnvParams):\n",
        "    \"\"\"Parameters for the rocket landing environment.\"\"\"\n",
        "    dt: float = 0.05\n",
        "    planet_radius: float = 50.\n",
        "    planet_mass: float = 5.0e14\n",
        "    rocket_max_thrust: float = 38000.0\n",
        "    rocket_max_gimbal: float = 0.3\n",
        "    rocket_mass: float = 2600.0\n",
        "    rocket_moment_of_inertia: float = 250.0\n",
        "    rocket_initial_fuel: float = 5000.0\n",
        "    rocket_fuel_consumption_rate: float = 100.\n",
        "    init_min_orbit_radius: float = 80.0\n",
        "    init_max_orbit_radius: float = 150.0\n",
        "    init_orbit_velocity_noise: float = 0.1\n",
        "    max_steps_in_episode: int = 1500\n",
        "    landing_max_speed: float = 2.0\n",
        "    landing_max_angle: float = 0.3\n",
        "    landing_max_omega: float = 0.5\n",
        "    landing_position_tolerance: float = jnp.pi/30\n",
        "    # Terminal rewards\n",
        "    reward_sl_cp_co_ls: float = 1000.0\n",
        "    reward_nsl_cp: float = 200.0\n",
        "    reward_nsl_ncp: float = 100.0\n",
        "    reward_sl_ncp_co_ls: float = 500.0\n",
        "    reward_out_of_bounds: float = -1000.0\n",
        "    reward_timeout: float = -2000.0\n",
        "    reward_retrograde_burn: float = 0.02\n",
        "\n",
        "def angle_normalize(x):\n",
        "    \"\"\"Normalize the angle - radians.\"\"\"\n",
        "    return ((x + jnp.pi) % (2 * jnp.pi)) - jnp.pi\n",
        "\n",
        "class RocketLander(environment.Environment[EnvState, EnvParams]):\n",
        "    \"\"\"JAX/Gymnax implementation of a 2D rocket landing environment.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @property\n",
        "    def default_params(self) -> EnvParams:\n",
        "        return EnvParams()\n",
        "\n",
        "    def step_env(self, key, state, action, params):\n",
        "        \"\"\"Integrate physics for one timestep.\"\"\"\n",
        "        r = jnp.sqrt(state.x**2 + state.y**2)\n",
        "        a_gravity_magnitude = G * params.planet_mass / r**2\n",
        "        a_g_x = -a_gravity_magnitude * state.x / r\n",
        "        a_g_y = -a_gravity_magnitude * state.y / r\n",
        "\n",
        "        throttle = jnp.clip(action[0], -1.0, 1.0)\n",
        "        gimbal = jnp.clip(action[1], -1.0, 1.0)\n",
        "        throttle = (throttle + 1.0) / 2.0\n",
        "        gimbal *= params.rocket_max_gimbal\n",
        "        throttle = jnp.minimum(throttle, state.fuel / (params.rocket_fuel_consumption_rate * params.dt))\n",
        "\n",
        "        thrust_angle = state.theta + gimbal\n",
        "        F_thrust = throttle * params.rocket_max_thrust\n",
        "        a_t_x = F_thrust * jnp.sin(thrust_angle) / params.rocket_mass\n",
        "        a_t_y = F_thrust * jnp.cos(thrust_angle) / params.rocket_mass\n",
        "        a_omega = F_thrust * jnp.sin(gimbal) / params.rocket_moment_of_inertia\n",
        "\n",
        "        a_x = a_g_x + a_t_x\n",
        "        a_y = a_g_y + a_t_y\n",
        "        dx = state.dx + a_x * params.dt\n",
        "        dy = state.dy + a_y * params.dt\n",
        "        omega = state.omega + a_omega * params.dt\n",
        "        x = state.x + dx * params.dt\n",
        "        y = state.y + dy * params.dt\n",
        "        theta = state.theta + omega * params.dt\n",
        "        fuel = jnp.maximum(0.0, state.fuel - throttle * params.rocket_fuel_consumption_rate * params.dt)\n",
        "\n",
        "        new_state = EnvState(\n",
        "            x=x, y=y, theta=theta, dx=dx, dy=dy, omega=omega,\n",
        "            throttle=throttle, gimbal=gimbal, fuel=fuel,\n",
        "            target_angle=state.target_angle, time=state.time + params.dt\n",
        "        )\n",
        "        done = self.is_terminal(new_state, params)\n",
        "        reward = self._compute_reward(state, new_state, params)\n",
        "        key, noise_key = jax.random.split(key)\n",
        "        obs = self.get_obs(new_state, params, noise_key)\n",
        "        return obs, new_state, reward, done, {}\n",
        "\n",
        "    def reset_env(self, key, params):\n",
        "        \"\"\"Reset environment by sampling initial orbit.\"\"\"\n",
        "        key, angle_key, orbit_radius_key, orbit_vel_noise_key, target_angle_key = jax.random.split(key, 5)\n",
        "        angle_init = jax.random.uniform(angle_key, minval=0.0, maxval=2.0 * jnp.pi)\n",
        "        orbit_radius_init = jax.random.uniform(orbit_radius_key, minval=params.init_min_orbit_radius, maxval=params.init_max_orbit_radius)\n",
        "        x_init = orbit_radius_init * jnp.cos(angle_init)\n",
        "        y_init = orbit_radius_init * jnp.sin(angle_init)\n",
        "        orbit_velocity_magnitude_init = jnp.sqrt(G * params.planet_mass / orbit_radius_init)\n",
        "        velocity_noise = params.init_orbit_velocity_noise * (2.0 * jax.random.uniform(orbit_vel_noise_key) - 1.0)\n",
        "        orbit_velocity_magnitude_init *= (1.0 + velocity_noise)\n",
        "        dx_init = -orbit_velocity_magnitude_init * jnp.sin(angle_init)\n",
        "        dy_init = orbit_velocity_magnitude_init * jnp.cos(angle_init)\n",
        "        theta_init = angle_init + (3/2) * jnp.pi\n",
        "        target_angle = jax.random.uniform(target_angle_key, minval=0.0, maxval=2.0 * jnp.pi)\n",
        "\n",
        "        state = EnvState(\n",
        "            x=x_init, y=y_init, theta=theta_init, dx=dx_init, dy=dy_init,\n",
        "            omega=jnp.array(0.0), throttle=jnp.array(0.0), gimbal=jnp.array(0.0),\n",
        "            fuel=jnp.array(params.rocket_initial_fuel), target_angle=target_angle, time=jnp.array(0)\n",
        "        )\n",
        "        return self.get_obs(state, params, key), state\n",
        "\n",
        "    def get_obs(self, state, params=None, key=None):\n",
        "        \"\"\"Get observation vector.\"\"\"\n",
        "        if params is None:\n",
        "            params = self.default_params\n",
        "        r = jnp.sqrt(state.x ** 2 + state.y ** 2)\n",
        "        altitude = r - params.planet_radius\n",
        "        positional_angle = jnp.arctan2(state.y, state.x)\n",
        "        delta_angle = angle_normalize(state.target_angle - positional_angle)\n",
        "        radial_vel = (state.x * state.dx + state.y * state.dy) / r\n",
        "        tangential_vel = (state.x * state.dy - state.y * state.dx) / r\n",
        "        theta_relative = angle_normalize(state.theta - positional_angle)\n",
        "        return jnp.array([altitude, delta_angle, radial_vel, tangential_vel, theta_relative, state.omega, state.throttle, state.gimbal, state.fuel])\n",
        "\n",
        "    def is_terminal(self, state, params):\n",
        "        \"\"\"Check whether state is terminal.\"\"\"\n",
        "        max_time = params.max_steps_in_episode * params.dt\n",
        "        timeout = state.time >= max_time\n",
        "        r = jnp.sqrt(state.x ** 2 + state.y ** 2)\n",
        "        landed = r <= params.planet_radius\n",
        "        escaped = r > params.planet_radius * 4\n",
        "        return jnp.array(landed | timeout | escaped)\n",
        "\n",
        "    def _compute_reward(self, old_state, new_state, params):\n",
        "        \"\"\"Compute reward with terminal rewards REPLACING shaping rewards.\"\"\"\n",
        "        r = jnp.sqrt(new_state.x ** 2 + new_state.y ** 2)\n",
        "        altitude = r - params.planet_radius\n",
        "        normalized_altitude = jnp.clip(altitude / (params.init_max_orbit_radius - params.planet_radius), 0.0, 2.0)\n",
        "        landed = r <= params.planet_radius\n",
        "        escaped = r > params.planet_radius * 4\n",
        "        max_time = params.max_steps_in_episode * params.dt\n",
        "        timeout = new_state.time >= max_time\n",
        "        is_terminal = landed | escaped | timeout\n",
        "\n",
        "        positional_angle = jnp.arctan2(new_state.y, new_state.x)\n",
        "        delta_angle = jnp.abs(angle_normalize(new_state.target_angle - positional_angle))\n",
        "        v_total = jnp.sqrt(new_state.dx ** 2 + new_state.dy ** 2)\n",
        "        v_tangential = (new_state.x * new_state.dy - new_state.y * new_state.dx) / r\n",
        "        theta_relative = angle_normalize(new_state.theta - positional_angle)\n",
        "\n",
        "        r_old = jnp.sqrt(old_state.x ** 2 + old_state.y ** 2)\n",
        "        normalized_altitude_old = jnp.clip((r_old - params.planet_radius) / (params.init_max_orbit_radius - params.planet_radius), 0.0, 2.0)\n",
        "        positional_angle_old = jnp.arctan2(old_state.y, old_state.x)\n",
        "        delta_angle_old = jnp.abs(angle_normalize(old_state.target_angle - positional_angle_old))\n",
        "        v_total_old = jnp.sqrt(old_state.dx ** 2 + old_state.dy ** 2)\n",
        "        v_tangential_old = (old_state.x * old_state.dy - old_state.y * old_state.dx) / r_old\n",
        "        theta_relative_old = jnp.abs(angle_normalize(old_state.theta - positional_angle_old))\n",
        "\n",
        "        slow = v_total < params.landing_max_speed\n",
        "        correct_position = delta_angle <= params.landing_position_tolerance\n",
        "        correct_orientation = jnp.abs(theta_relative) <= params.landing_max_angle\n",
        "        low_spin = jnp.abs(new_state.omega) <= params.landing_max_omega\n",
        "\n",
        "        # Terminal rewards\n",
        "        terminal_reward = jnp.array(0.0)\n",
        "        sl_cp_co_ls = landed & slow & correct_position & correct_orientation & low_spin\n",
        "        terminal_reward = jnp.where(sl_cp_co_ls, params.reward_sl_cp_co_ls, terminal_reward)\n",
        "        sl_ncp_co_ls = landed & slow & ~correct_position & correct_orientation & low_spin\n",
        "        terminal_reward = jnp.where(sl_ncp_co_ls, params.reward_sl_ncp_co_ls * jnp.exp(-delta_angle * 2.0), terminal_reward)\n",
        "        sl_nco_nls = landed & slow & ~(correct_orientation & low_spin)\n",
        "        terminal_reward = jnp.where(sl_nco_nls, 100.0 * jnp.exp(-delta_angle), terminal_reward)\n",
        "        nsl_cp = landed & ~slow & correct_position\n",
        "        terminal_reward = jnp.where(nsl_cp, params.reward_nsl_cp, terminal_reward)\n",
        "        nsl_ncp = landed & ~slow & ~correct_position\n",
        "        terminal_reward = jnp.where(nsl_ncp, params.reward_nsl_ncp, terminal_reward)\n",
        "        terminal_reward = jnp.where(escaped, params.reward_out_of_bounds, terminal_reward)\n",
        "        terminal_reward = jnp.where(timeout & ~landed & ~escaped, params.reward_timeout, terminal_reward)\n",
        "\n",
        "        ### Non-terminal (shaping) rewards\n",
        "        shaping_reward = jnp.array(0.0)\n",
        "\n",
        "        # Reward for firing engine while pointed retrograde (opposite to velocity direction)\n",
        "        # This encourages braking burns to slow down\n",
        "        thrust_angle = new_state.theta + new_state.gimbal\n",
        "        velocity_angle = jnp.arctan2(new_state.dy, new_state.dx)\n",
        "        alignment = jnp.cos(thrust_angle - velocity_angle)  # 1 = prograde, -1 = retrograde\n",
        "        retrograde_alignment = (1.0 - alignment) / 2.0  # 0 = prograde, 1 = retrograde\n",
        "        throttle_used = new_state.throttle > 0.1\n",
        "        velocity_factor = jnp.minimum(v_total / 10.0, 1.0)\n",
        "        retrograde_burn_reward = params.reward_retrograde_burn * retrograde_alignment * throttle_used * velocity_factor\n",
        "        shaping_reward += retrograde_burn_reward\n",
        "\n",
        "        # Reward for decreasing tangential velocity (we want v_tangential -> 0)\n",
        "        # Negative coefficient because we want to reward when (new - old) is negative\n",
        "        tangential_velocity_change = jnp.abs(v_tangential) - jnp.abs(v_tangential_old)\n",
        "        shaping_reward += -0.3 * tangential_velocity_change\n",
        "\n",
        "        # Reward for decreasing altitude, but only when near the target landing spot\n",
        "        # Weight by exp(-delta_angle) so we don't reward descending on the wrong side\n",
        "        delta_angle_factor = jnp.exp(-delta_angle * 3)\n",
        "        altitude_change = normalized_altitude - normalized_altitude_old\n",
        "        shaping_reward += -1.0 * delta_angle_factor * altitude_change\n",
        "\n",
        "        # Reward for improving orientation (theta_relative -> 0), but only when near landing\n",
        "        # near_landing_factor is high only when both altitude and velocity are low\n",
        "        near_landing_factor = jnp.exp(-normalized_altitude * 3.0) * jnp.exp(-v_total)\n",
        "        theta_relative_change = jnp.abs(theta_relative) - jnp.abs(theta_relative_old)\n",
        "        shaping_reward += -1.0 * near_landing_factor * theta_relative_change\n",
        "\n",
        "        # Reward for decreasing angular velocity (omega -> 0), but only when near landing\n",
        "        omega_change = jnp.abs(new_state.omega) - jnp.abs(old_state.omega)\n",
        "        shaping_reward += -0.1 * near_landing_factor * omega_change\n",
        "\n",
        "        # Bonus for being in the \"landing zone\" (low altitude AND near target)\n",
        "        # This is a constant bonus per timestep, not a change-based reward\n",
        "        low_altitude_factor = jnp.exp(-normalized_altitude * 2.0)\n",
        "        near_target_factor = jnp.exp(-delta_angle * 2.0)\n",
        "        landing_zone_bonus = 0.1 * low_altitude_factor * near_target_factor * near_target_factor\n",
        "        shaping_reward += landing_zone_bonus\n",
        "\n",
        "        # Reward for decreasing total velocity (we want v_total -> 0)\n",
        "        # Only reward decreases, don't punish increases (use max with 0)\n",
        "        velocity_change = v_total - v_total_old\n",
        "        shaping_reward += -0.1 * jnp.maximum(0.0, -velocity_change)  # reward when velocity_change < 0\n",
        "\n",
        "        # Punish high angular velocity (spinning too fast)\n",
        "        high_omega = jnp.abs(new_state.omega) > 2.0\n",
        "        angular_velocity_punishment = 0.1 * high_omega * (jnp.abs(new_state.omega) - 2.0)\n",
        "        shaping_reward -= angular_velocity_punishment\n",
        "\n",
        "        # Constant penalty for being far from target (angular distance)\n",
        "        # This discourages taking the \"long way around\" the planet\n",
        "        distance_from_target_penalty = 0.02 * delta_angle\n",
        "        shaping_reward -= distance_from_target_penalty\n",
        "\n",
        "        return jnp.where(is_terminal, terminal_reward, shaping_reward)\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return \"RocketLander\"\n",
        "\n",
        "    @property\n",
        "    def num_actions(self):\n",
        "        return 2\n",
        "\n",
        "    def action_space(self, params=None):\n",
        "        return spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=jnp.float32)\n",
        "\n",
        "    def observation_space(self, params=None):\n",
        "        low = jnp.array([-10.0, -jnp.pi, -50.0, -50.0, -jnp.pi, -10.0, 0.0, -0.5, 0.0])\n",
        "        high = jnp.array([150.0, jnp.pi, 50.0, 50.0, jnp.pi, 10.0, 1.0, 0.5, 5000.0])\n",
        "        return spaces.Box(low, high, shape=(9,), dtype=jnp.float32)\n",
        "\n",
        "print(\"âœ… Environment defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Visualization utilities.\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "def visualize_trajectory(states, params, title=\"Rocket Landing Trajectory\", save_path=None):\n",
        "    \"\"\"Visualize the rocket trajectory around the planet.\"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "\n",
        "    # Draw planet\n",
        "    theta = np.linspace(0, 2 * np.pi, 100)\n",
        "    planet_x = params.planet_radius * np.cos(theta)\n",
        "    planet_y = params.planet_radius * np.sin(theta)\n",
        "    ax.fill(planet_x, planet_y, color='#3d5a80', alpha=0.8, label='Planet')\n",
        "    ax.plot(planet_x, planet_y, color='#1d3557', linewidth=2)\n",
        "\n",
        "    # Extract trajectory\n",
        "    xs = np.array([float(s.x) for s in states])\n",
        "    ys = np.array([float(s.y) for s in states])\n",
        "    throttles = np.array([float(s.throttle) for s in states])\n",
        "\n",
        "    # Draw trajectory colored by throttle\n",
        "    points = np.array([xs, ys]).T.reshape(-1, 1, 2)\n",
        "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "    norm = plt.Normalize(0, 1)\n",
        "    lc = LineCollection(segments, cmap='YlOrRd', norm=norm, linewidth=2, alpha=0.8)\n",
        "    lc.set_array(throttles[:-1])\n",
        "    ax.add_collection(lc)\n",
        "    cbar = plt.colorbar(lc, ax=ax, label='Throttle')\n",
        "    cbar.ax.yaxis.set_tick_params(color='white')\n",
        "    cbar.ax.yaxis.label.set_color('white')\n",
        "    plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color='white')\n",
        "\n",
        "    # Markers\n",
        "    ax.scatter(xs[0], ys[0], s=100, c='green', marker='o', zorder=5, label='Start')\n",
        "    ax.scatter(xs[-1], ys[-1], s=100, c='red', marker='x', zorder=5, label='End')\n",
        "\n",
        "    # Target\n",
        "    target_angle = float(states[0].target_angle)\n",
        "    target_x = params.planet_radius * np.cos(target_angle)\n",
        "    target_y = params.planet_radius * np.sin(target_angle)\n",
        "    ax.scatter(target_x, target_y, s=200, c='yellow', marker='*', zorder=5, edgecolors='black', label='Target')\n",
        "\n",
        "    # Rocket orientation arrows\n",
        "    n_arrows = min(30, len(states))\n",
        "    arrow_indices = np.linspace(0, len(states) - 1, n_arrows, dtype=int)\n",
        "    for idx in arrow_indices:\n",
        "        s = states[idx]\n",
        "        x, y = float(s.x), float(s.y)\n",
        "        theta = float(s.theta)\n",
        "        dx = np.sin(theta) * 8\n",
        "        dy = np.cos(theta) * 8\n",
        "        ax.arrow(x, y, dx, dy, head_width=4, head_length=2, fc='#cccccc', ec='white', alpha=0.9)\n",
        "\n",
        "    # Styling\n",
        "    fov = params.init_max_orbit_radius * 1.2\n",
        "    ax.set_xlim(-fov, fov)\n",
        "    ax.set_ylim(-fov, fov)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlabel('X Position')\n",
        "    ax.set_ylabel('Y Position')\n",
        "    ax.set_title(title)\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_facecolor('#0d1b2a')\n",
        "    fig.patch.set_facecolor('#0d1b2a')\n",
        "    ax.tick_params(colors='white')\n",
        "    ax.xaxis.label.set_color('white')\n",
        "    ax.yaxis.label.set_color('white')\n",
        "    ax.title.set_color('white')\n",
        "    ax.legend(facecolor='#1b263b', edgecolor='white', labelcolor='white')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, facecolor=fig.get_facecolor())\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "    return fig\n",
        "\n",
        "print(\"âœ… Visualization utilities defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Policy and Training\n",
        "\n",
        "We use REINFORCE with a Gaussian policy to train the rocket landing agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Training components.\"\"\"\n",
        "import jax.random as jr\n",
        "from jax.random import PRNGKey\n",
        "import optax\n",
        "\n",
        "# Create the environment\n",
        "env = RocketLander()\n",
        "env_params = env.default_params\n",
        "\n",
        "def initialize_mlp(layer_sizes, key, scale=1e-2):\n",
        "    \"\"\"Initialize MLP parameters.\"\"\"\n",
        "    keys = jr.split(key, 2 * len(layer_sizes))\n",
        "    params = []\n",
        "    for i in range(len(layer_sizes) - 1):\n",
        "        input_size, output_size = layer_sizes[i], layer_sizes[i + 1]\n",
        "        W = jr.normal(keys[2 * i], (input_size, output_size)) * scale\n",
        "        b = jr.normal(keys[2 * i + 1], (output_size,)) * scale\n",
        "        params.append((W, b))\n",
        "    return params\n",
        "\n",
        "def policy(params, x):\n",
        "    \"\"\"Gaussian policy network.\"\"\"\n",
        "    for (W, b) in params[:-1]:\n",
        "        x = jnp.dot(x, W) + b\n",
        "        x = jax.nn.relu(x)\n",
        "    W, b = params[-1]\n",
        "    output = jnp.dot(x, W) + b\n",
        "    action_dim = output.shape[-1] // 2\n",
        "    mean = jnp.tanh(output[:action_dim])\n",
        "    log_std = jnp.clip(output[action_dim:], -2.0, 0.5)\n",
        "    return mean, log_std\n",
        "\n",
        "def get_action(params, x, key):\n",
        "    \"\"\"Sample an action from the Gaussian policy.\"\"\"\n",
        "    mean, log_std = policy(params, x)\n",
        "    std = jnp.exp(log_std)\n",
        "    noise = jr.normal(key, shape=mean.shape)\n",
        "    action = jnp.clip(mean + std * noise, -1.0, 1.0)\n",
        "    return action, mean, log_std\n",
        "\n",
        "def get_log_prob(params, x, action):\n",
        "    \"\"\"Compute log probability of action under the Gaussian policy.\"\"\"\n",
        "    mean, log_std = policy(params, x)\n",
        "    std = jnp.exp(log_std)\n",
        "    return -0.5 * jnp.sum(((action - mean) / std) ** 2 + 2 * log_std + jnp.log(2 * jnp.pi))\n",
        "\n",
        "@jax.jit\n",
        "def update_delta(delta, grad_theta):\n",
        "    \"\"\"Update the parameter gradients.\"\"\"\n",
        "    return jax.tree.map(lambda x, y: x + y, delta, grad_theta), None\n",
        "\n",
        "def rollout(params, env_params, rng_input, steps_in_episode):\n",
        "    \"\"\"Rollout an episode using the policy.\"\"\"\n",
        "    rng_reset, rng_episode = jr.split(rng_input)\n",
        "    obs, state = env.reset_env(rng_reset, env_params)\n",
        "\n",
        "    def policy_step(state_input, tmp):\n",
        "        obs, state, rng = state_input\n",
        "        rng, rng_action, rng_step = jr.split(rng, 3)\n",
        "        action, mean, log_std = get_action(params, obs, rng_action)\n",
        "        next_obs, next_state, reward, done, _ = env.step_env(rng_step, state, action, env_params)\n",
        "        carry = [next_obs, next_state, rng]\n",
        "        return carry, [obs, state, action, reward, next_obs, done]\n",
        "\n",
        "    _, scan_out = jax.lax.scan(policy_step, [obs, state, rng_episode], (), length=steps_in_episode)\n",
        "    return scan_out\n",
        "\n",
        "jit_rollout = jax.jit(rollout, static_argnums=3)\n",
        "\n",
        "def loss_REINFORCE(params, obs, action, reward, baseline, gamma=0.99):\n",
        "    \"\"\"Compute REINFORCE loss with baseline.\"\"\"\n",
        "    def trajectory_gradients(reward, obs, action, baseline, delta):\n",
        "        G_init = 0.0\n",
        "        def step(carry, variables):\n",
        "            G, delta = carry\n",
        "            r, obs, action, b = variables\n",
        "            G = gamma * G + r\n",
        "            advantage = G - b\n",
        "            def neg_log_prob(params):\n",
        "                return -get_log_prob(params, obs, action)\n",
        "            grad_delta = jax.grad(neg_log_prob)(params)\n",
        "            grad_delta = jax.tree.map(lambda gd: gd * advantage, grad_delta)\n",
        "            delta, _ = update_delta(delta, grad_delta)\n",
        "            return (G, delta), G\n",
        "        variables = (reward[::-1], obs[::-1], action[::-1], baseline[::-1])\n",
        "        (_, delta), Gt = jax.lax.scan(step, (G_init, delta), variables)\n",
        "        return delta, Gt\n",
        "\n",
        "    parallel_trajectory_gradients = jax.vmap(trajectory_gradients, in_axes=(0, 0, 0, None, None))\n",
        "    delta = jax.tree.map(lambda t: jnp.zeros(t.shape), params)\n",
        "    deltas, Gs = parallel_trajectory_gradients(reward, obs, action, baseline, delta)\n",
        "    delta, _ = jax.lax.scan(update_delta, delta, deltas)\n",
        "    return delta, jnp.array(Gs)\n",
        "\n",
        "loss_REINFORCE = jax.jit(loss_REINFORCE)\n",
        "\n",
        "def mean_baseline(Gs):\n",
        "    \"\"\"Compute constant baseline as mean of discounted returns.\"\"\"\n",
        "    T = Gs.shape[1]\n",
        "    return jnp.ones((T,)) * jnp.mean(Gs)\n",
        "\n",
        "print(\"âœ… Training components defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    num_iters=5000,\n",
        "    steps_in_episode=1500,\n",
        "    lr=0.0002,\n",
        "    gamma=0.995,\n",
        "    n_batches=128,\n",
        "    hidden_size=256,\n",
        "    n_hidden_layers=3,\n",
        "    visualize_every=500,\n",
        "    seed=42,\n",
        "    grad_clip=0.5,\n",
        "):\n",
        "    \"\"\"Train the rocket landing policy using REINFORCE.\"\"\"\n",
        "    key = PRNGKey(seed)\n",
        "    \n",
        "    # Network architecture\n",
        "    obs_dim = 9\n",
        "    action_dim = 2\n",
        "    hidden_layers = tuple([hidden_size] * n_hidden_layers)\n",
        "    layer_sizes = (obs_dim,) + hidden_layers + (2 * action_dim,)\n",
        "    \n",
        "    # Initialize\n",
        "    key, subkey = jr.split(key)\n",
        "    params = initialize_mlp(layer_sizes, key=subkey)\n",
        "    optim = optax.chain(optax.clip_by_global_norm(grad_clip), optax.adam(learning_rate=lr))\n",
        "    opt_state = optim.init(params)\n",
        "    Gs = jnp.zeros((n_batches, steps_in_episode))\n",
        "    \n",
        "    key, subkey = jr.split(key)\n",
        "    iter_keys = jr.split(subkey, num_iters)\n",
        "    parallel_rollout = jax.vmap(rollout, in_axes=(None, None, 0, None))\n",
        "    \n",
        "    def step(carry, key):\n",
        "        params, opt_state, Gs = carry\n",
        "        keys = jr.split(key, n_batches)\n",
        "        obs, state, action, reward, next_obs, done = parallel_rollout(params, env_params, keys, steps_in_episode)\n",
        "        baseline = mean_baseline(Gs)\n",
        "        delta, Gs_new = loss_REINFORCE(params, obs, action, reward, baseline, gamma)\n",
        "        updates, opt_state = optim.update(delta, opt_state, params)\n",
        "        new_params = optax.apply_updates(params, updates)\n",
        "        mean_reward = jnp.mean(jnp.sum(reward, axis=-1))\n",
        "        return (new_params, opt_state, Gs_new), mean_reward\n",
        "\n",
        "    step = jax.jit(step)\n",
        "    \n",
        "    history = []\n",
        "    current_params = params\n",
        "    current_opt_state = opt_state\n",
        "    current_Gs = Gs\n",
        "    \n",
        "    print(f\"ðŸš€ Starting training for {num_iters} iterations...\")\n",
        "    print(f\"   Network: {layer_sizes}\")\n",
        "    print(f\"   Batch size: {n_batches}, Steps: {steps_in_episode}\")\n",
        "    print(f\"   LR: {lr}, Gamma: {gamma}\")\n",
        "    print()\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "        (current_params, current_opt_state, current_Gs), mean_reward = step(\n",
        "            (current_params, current_opt_state, current_Gs), iter_keys[i]\n",
        "        )\n",
        "        history.append(float(mean_reward))\n",
        "        \n",
        "        if (i + 1) % 100 == 0:\n",
        "            avg_reward = jnp.mean(jnp.array(history[-100:]))\n",
        "            print(f\"Iteration {i + 1}/{num_iters} | Avg Reward (last 100): {avg_reward:.2f}\")\n",
        "        \n",
        "        if (i + 1) % visualize_every == 0:\n",
        "            print(f\"\\nðŸ“Š Visualizing at iteration {i + 1}...\")\n",
        "            key, vis_key = jr.split(key)\n",
        "            visualize_policy(current_params, vis_key, title=f\"Iteration {i + 1}\")\n",
        "    \n",
        "    print(\"\\nâœ… Training complete!\")\n",
        "    key, vis_key = jr.split(key)\n",
        "    visualize_policy(current_params, vis_key, title=\"Final Policy\")\n",
        "    \n",
        "    # Plot training curve\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(history, alpha=0.3, label='Episode Reward')\n",
        "    window = min(100, len(history) // 10)\n",
        "    if window > 1:\n",
        "        smoothed = jnp.convolve(jnp.array(history), jnp.ones(window) / window, mode='valid')\n",
        "        plt.plot(range(window - 1, len(history)), smoothed, label=f'Smoothed (window={window})')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Mean Episode Reward')\n",
        "    plt.title('Training Progress')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return current_params, history\n",
        "\n",
        "\n",
        "def visualize_policy(params, key, title=\"Policy Trajectory\"):\n",
        "    \"\"\"Run a single episode and visualize the trajectory.\"\"\"\n",
        "    obs, state, action, reward, next_obs, done = rollout(params, env_params, key, env_params.max_steps_in_episode)\n",
        "    states = []\n",
        "    for i in range(len(state.x)):\n",
        "        s = EnvState(\n",
        "            x=state.x[i], y=state.y[i], theta=state.theta[i],\n",
        "            dx=state.dx[i], dy=state.dy[i], omega=state.omega[i],\n",
        "            throttle=state.throttle[i], gimbal=state.gimbal[i],\n",
        "            fuel=state.fuel[i], target_angle=state.target_angle[i], time=state.time[i],\n",
        "        )\n",
        "        states.append(s)\n",
        "        if done[i]:\n",
        "            break\n",
        "    \n",
        "    # Only sum rewards up to the terminal step (after that, rewards are garbage due to r=0)\n",
        "    episode_length = len(states)\n",
        "    total_reward = float(jnp.sum(reward[:episode_length]))\n",
        "    print(f\"  Total reward: {total_reward:.2f}, Steps: {episode_length}\")\n",
        "    visualize_trajectory(states, env_params, title=f\"{title} (R={total_reward:.1f})\")\n",
        "\n",
        "print(\"âœ… Training function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ® Run Training!\n",
        "\n",
        "Adjust the parameters below and run training. With GPU, this should be quite fast!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the agent!\n",
        "# Adjust num_iters for longer/shorter training\n",
        "\n",
        "params, history = train(\n",
        "    num_iters=10000,          # Number of training iterations\n",
        "    steps_in_episode=1500,    # Max steps per episode\n",
        "    lr=0.0002,                # Learning rate\n",
        "    gamma=0.995,              # Discount factor\n",
        "    n_batches=128,            # Parallel episodes per iteration\n",
        "    hidden_size=256,          # Hidden layer size\n",
        "    n_hidden_layers=3,        # Number of hidden layers\n",
        "    visualize_every=1000,     # Visualize every N iterations\n",
        "    seed=42,\n",
        "    grad_clip=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Trained Policy\n",
        "\n",
        "Run multiple episodes to see how the trained agent performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained policy multiple times\n",
        "key = jr.PRNGKey(123)\n",
        "\n",
        "for i in range(5):\n",
        "    key, subkey = jr.split(key)\n",
        "    print(f\"\\n--- Episode {i+1} ---\")\n",
        "    visualize_policy(params, subkey, title=f\"Test Episode {i+1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save/Load Model\n",
        "\n",
        "You can save the trained parameters to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model (optional)\n",
        "import pickle\n",
        "\n",
        "# Flatten params for saving\n",
        "params_np = jax.tree.map(lambda x: np.array(x), params)\n",
        "\n",
        "with open('rocket_lander_params.pkl', 'wb') as f:\n",
        "    pickle.dump(params_np, f)\n",
        "\n",
        "print(\"Model saved to rocket_lander_params.pkl\")\n",
        "\n",
        "# To load:\n",
        "# with open('rocket_lander_params.pkl', 'rb') as f:\n",
        "#     params_loaded = pickle.load(f)\n",
        "# params_loaded = jax.tree.map(lambda x: jnp.array(x), params_loaded)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

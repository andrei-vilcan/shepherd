{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Rocket Landing with REINFORCE\n",
        "\n",
        "This notebook trains an AI agent to land a rocket on a planet using the REINFORCE algorithm.\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q jax[cuda12] flax optax gymnax matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify JAX is using GPU\n",
        "import jax\n",
        "print(f\"JAX devices: {jax.devices()}\")\n",
        "print(f\"Default backend: {jax.default_backend()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Definition\n",
        "\n",
        "The rocket landing environment simulates a 2D orbital mechanics scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Rocket Landing with REINFORCE\n",
        "\n",
        "This notebook trains a neural network policy to land a rocket on a planet using the REINFORCE algorithm with a Gaussian policy.\n",
        "\n",
        "**Features:**\n",
        "- JAX-based physics simulation\n",
        "- Vectorized environment for fast training\n",
        "- Gaussian policy for continuous control\n",
        "- Reward shaping to guide learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q jax[cuda12] flax optax gymnax matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pickle\n",
        "from typing import Any, Optional, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "from jax.random import PRNGKey\n",
        "import optax\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.collections import LineCollection\n",
        "from flax import struct\n",
        "from gymnax.environments import environment, spaces\n",
        "\n",
        "# Verify JAX is using GPU\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"JAX backend: {jax.default_backend()}\")\n",
        "print(f\"JAX devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Environment Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gravitational constant\n",
        "G = 6.67430e-11\n",
        "\n",
        "\n",
        "def angle_normalize(x: jax.Array) -> jax.Array:\n",
        "    \"\"\"Normalize the angle - radians.\"\"\"\n",
        "    return ((x + jnp.pi) % (2 * jnp.pi)) - jnp.pi\n",
        "\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvState(environment.EnvState):\n",
        "    \"\"\"State of the rocket landing environment.\"\"\"\n",
        "    # position\n",
        "    x: jnp.ndarray\n",
        "    y: jnp.ndarray\n",
        "    theta: jnp.ndarray\n",
        "\n",
        "    # velocity\n",
        "    dx: jnp.ndarray\n",
        "    dy: jnp.ndarray\n",
        "    omega: jnp.ndarray\n",
        "\n",
        "    # engine\n",
        "    throttle: jnp.ndarray\n",
        "    gimbal: jnp.ndarray\n",
        "\n",
        "    # fuel\n",
        "    fuel: jnp.ndarray\n",
        "\n",
        "    # target landing angle\n",
        "    target_angle: jnp.ndarray\n",
        "\n",
        "    # timestep\n",
        "    time: jnp.ndarray\n",
        "\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvParams(environment.EnvParams):\n",
        "    \"\"\"Parameters for the rocket landing environment.\"\"\"\n",
        "    max_steps_in_episode: int = 1500\n",
        "    dt: float = 0.05\n",
        "\n",
        "    # planet properties\n",
        "    planet_radius: float = 50.0\n",
        "    planet_mass: float = 5.0e14\n",
        "\n",
        "    # rocket properties\n",
        "    rocket_max_thrust: float = 38000.0\n",
        "    rocket_max_gimbal: float = 0.3\n",
        "    rocket_mass: float = 2600.0\n",
        "    rocket_moment_of_inertia: float = 250.0\n",
        "    rocket_initial_fuel: float = 5000.0\n",
        "    rocket_fuel_consumption_rate: float = 100.\n",
        "\n",
        "    # initialization parameters\n",
        "    init_min_orbit_radius: float = 80.0\n",
        "    init_max_orbit_radius: float = 150.0\n",
        "    init_orbit_velocity_noise: float = 0.1\n",
        "\n",
        "    # good landing thresholds\n",
        "    landing_max_speed: float = 2.0\n",
        "    landing_max_angle: float = 0.3\n",
        "    landing_max_omega: float = 0.5\n",
        "    landing_position_tolerance: float = jnp.pi/30\n",
        "\n",
        "    # terminal rewards\n",
        "    reward_landed: float = 100.0\n",
        "    reward_sl_cp_co_ls: float = 1000.0\n",
        "    reward_nsl_cp: float = 200.0\n",
        "    reward_nsl_ncp: float = 100.0\n",
        "    reward_sl_ncp_co_ls: float = 500.0\n",
        "    reward_out_of_fuel: float = -50.0\n",
        "    reward_out_of_bounds: float = -1000.0\n",
        "    reward_timeout: float = -2000.0\n",
        "\n",
        "    # non-terminal rewards\n",
        "    reward_retrograde_burn: float = 0.02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RocketLander(environment.Environment[EnvState, EnvParams]):\n",
        "    \"\"\"JAX/Gymnax implementation of a 2D rocket landing environment.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @property\n",
        "    def default_params(self) -> EnvParams:\n",
        "        return EnvParams()\n",
        "\n",
        "    def step_env(self, key: jax.Array, state: EnvState, action: jax.Array, params: EnvParams):\n",
        "        \"\"\"Integrate physics for one timestep.\"\"\"\n",
        "        # Gravity\n",
        "        r = jnp.sqrt(state.x**2 + state.y**2)\n",
        "        a_gravity_magnitude = G * params.planet_mass / r**2\n",
        "        a_g_x = -a_gravity_magnitude * state.x / r\n",
        "        a_g_y = -a_gravity_magnitude * state.y / r\n",
        "\n",
        "        # Throttle and gimbal control\n",
        "        throttle = jnp.clip(action[0], -1.0, 1.0)\n",
        "        gimbal = jnp.clip(action[1], -1.0, 1.0)\n",
        "        throttle = (throttle + 1.0)/2.0\n",
        "        gimbal *= params.rocket_max_gimbal\n",
        "        throttle = jnp.minimum(throttle, state.fuel / (params.rocket_fuel_consumption_rate * params.dt))\n",
        "\n",
        "        # Thrust\n",
        "        thrust_angle = state.theta + gimbal\n",
        "        F_thrust = throttle * params.rocket_max_thrust\n",
        "        a_t_x = F_thrust * jnp.sin(thrust_angle) / params.rocket_mass\n",
        "        a_t_y = F_thrust * jnp.cos(thrust_angle) / params.rocket_mass\n",
        "        F_torque = F_thrust * jnp.sin(gimbal)\n",
        "        a_omega = F_torque / params.rocket_moment_of_inertia\n",
        "\n",
        "        # Physics integration\n",
        "        a_x, a_y = a_g_x + a_t_x, a_g_y + a_t_y\n",
        "        dx = state.dx + a_x * params.dt\n",
        "        dy = state.dy + a_y * params.dt\n",
        "        omega = state.omega + a_omega * params.dt\n",
        "        x = state.x + dx * params.dt\n",
        "        y = state.y + dy * params.dt\n",
        "        theta = state.theta + omega * params.dt\n",
        "        fuel = jnp.maximum(0.0, state.fuel - throttle * params.rocket_fuel_consumption_rate * params.dt)\n",
        "\n",
        "        new_state = EnvState(x=x, y=y, theta=theta, dx=dx, dy=dy, omega=omega,\n",
        "                             throttle=throttle, gimbal=gimbal, fuel=fuel,\n",
        "                             target_angle=state.target_angle, time=state.time+params.dt)\n",
        "        \n",
        "        done = self.is_terminal(new_state, params)\n",
        "        reward = self._compute_reward(state, new_state, params)\n",
        "        key, noise_key = jax.random.split(key)\n",
        "        obs = self.get_obs(new_state, params, noise_key)\n",
        "        return obs, new_state, reward, done, {}\n",
        "\n",
        "    def reset_env(self, key: jax.Array, params: EnvParams):\n",
        "        \"\"\"Reset environment by sampling initial orbit.\"\"\"\n",
        "        key, angle_key, orbit_radius_key, orbit_vel_noise_key, target_angle_key = jax.random.split(key, 5)\n",
        "        \n",
        "        angle_init = jax.random.uniform(angle_key, minval=0.0, maxval=2.0 * jnp.pi)\n",
        "        orbit_radius_init = jax.random.uniform(orbit_radius_key, minval=params.init_min_orbit_radius, maxval=params.init_max_orbit_radius)\n",
        "        \n",
        "        x_init = orbit_radius_init * jnp.cos(angle_init)\n",
        "        y_init = orbit_radius_init * jnp.sin(angle_init)\n",
        "        \n",
        "        orbit_velocity_magnitude_init = jnp.sqrt(G * params.planet_mass / orbit_radius_init)\n",
        "        velocity_noise = params.init_orbit_velocity_noise * (2.0 * jax.random.uniform(orbit_vel_noise_key) - 1.0)\n",
        "        orbit_velocity_magnitude_init *= (1.0 + velocity_noise)\n",
        "        \n",
        "        dx_init = -orbit_velocity_magnitude_init * jnp.sin(angle_init)\n",
        "        dy_init = orbit_velocity_magnitude_init * jnp.cos(angle_init)\n",
        "        theta_init = angle_init + (3/2)*jnp.pi\n",
        "        target_angle = jax.random.uniform(target_angle_key, minval=0.0, maxval=2.0 * jnp.pi)\n",
        "\n",
        "        state = EnvState(x=x_init, y=y_init, theta=theta_init, dx=dx_init, dy=dy_init,\n",
        "                         omega=jnp.array(0.0), throttle=jnp.array(0.0), gimbal=jnp.array(0.0),\n",
        "                         fuel=jnp.array(params.rocket_initial_fuel), target_angle=target_angle, time=jnp.array(0))\n",
        "        return self.get_obs(state, params, key), state\n",
        "\n",
        "    def get_obs(self, state: EnvState, params: EnvParams=None, key: jax.Array=None) -> jax.Array:\n",
        "        \"\"\"Get observation from state.\"\"\"\n",
        "        if params is None:\n",
        "            params = self.default_params\n",
        "        r = jnp.sqrt(state.x ** 2 + state.y ** 2)\n",
        "        altitude = r - params.planet_radius\n",
        "        positional_angle = jnp.arctan2(state.y, state.x)\n",
        "        delta_angle = angle_normalize(state.target_angle - positional_angle)\n",
        "        radial_vel = (state.x * state.dx + state.y * state.dy) / r\n",
        "        tangential_vel = (state.x * state.dy - state.y * state.dx) / r\n",
        "        theta_relative = angle_normalize(state.theta - positional_angle)\n",
        "        return jnp.array([altitude, delta_angle, radial_vel, tangential_vel,\n",
        "                          theta_relative, state.omega, state.throttle, state.gimbal, state.fuel])\n",
        "\n",
        "    def is_terminal(self, state: EnvState, params: EnvParams) -> jax.Array:\n",
        "        \"\"\"Check whether state is terminal.\"\"\"\n",
        "        max_time = params.max_steps_in_episode * params.dt\n",
        "        timeout = state.time >= max_time\n",
        "        r = jnp.sqrt(state.x ** 2 + state.y ** 2)\n",
        "        landed = r <= params.planet_radius\n",
        "        escaped = r > params.planet_radius * 4\n",
        "        return jnp.array(landed | timeout | escaped)\n",
        "\n",
        "    def _compute_reward(self, old_state: EnvState, new_state: EnvState, params: EnvParams) -> jax.Array:\n",
        "        \"\"\"Compute reward with shaping.\"\"\"\n",
        "        # New state calculations\n",
        "        r = jnp.sqrt(new_state.x ** 2 + new_state.y ** 2)\n",
        "        altitude = r - params.planet_radius\n",
        "        normalized_altitude = jnp.clip(altitude / (params.init_max_orbit_radius - params.planet_radius), 0.0, 2.0)\n",
        "\n",
        "        landed = r <= params.planet_radius\n",
        "        escaped = r > params.planet_radius * 4\n",
        "        max_time = params.max_steps_in_episode * params.dt\n",
        "        timeout = new_state.time >= max_time\n",
        "        is_terminal = landed | escaped | timeout\n",
        "\n",
        "        positional_angle = jnp.arctan2(new_state.y, new_state.x)\n",
        "        delta_angle = jnp.abs(angle_normalize(new_state.target_angle - positional_angle))\n",
        "        v_total = jnp.sqrt(new_state.dx ** 2 + new_state.dy ** 2)\n",
        "        v_tangential = (new_state.x * new_state.dy - new_state.y * new_state.dx) / r\n",
        "        theta_relative = angle_normalize(new_state.theta - positional_angle)\n",
        "\n",
        "        # Old state calculations\n",
        "        r_old = jnp.sqrt(old_state.x ** 2 + old_state.y ** 2)\n",
        "        normalized_altitude_old = jnp.clip((r_old - params.planet_radius) / (params.init_max_orbit_radius - params.planet_radius), 0.0, 2.0)\n",
        "        positional_angle_old = jnp.arctan2(old_state.y, old_state.x)\n",
        "        v_total_old = jnp.sqrt(old_state.dx ** 2 + old_state.dy ** 2)\n",
        "        v_tangential_old = (old_state.x * old_state.dy - old_state.y * old_state.dx) / r_old\n",
        "        theta_relative_old = jnp.abs(angle_normalize(old_state.theta - positional_angle_old))\n",
        "\n",
        "        # Landing conditions\n",
        "        slow = v_total < params.landing_max_speed\n",
        "        correct_position = delta_angle <= params.landing_position_tolerance\n",
        "        correct_orientation = jnp.abs(theta_relative) <= params.landing_max_angle\n",
        "        low_spin = jnp.abs(new_state.omega) <= params.landing_max_omega\n",
        "\n",
        "        # Terminal rewards\n",
        "        terminal_reward = jnp.array(0.0)\n",
        "        sl_cp_co_ls = landed & slow & correct_position & correct_orientation & low_spin\n",
        "        terminal_reward = jnp.where(sl_cp_co_ls, params.reward_sl_cp_co_ls, terminal_reward)\n",
        "        sl_ncp_co_ls = landed & slow & ~correct_position & correct_orientation & low_spin\n",
        "        terminal_reward = jnp.where(sl_ncp_co_ls, params.reward_sl_ncp_co_ls * jnp.exp(-delta_angle * 2.0), terminal_reward)\n",
        "        sl_nco_nls = landed & slow & ~(correct_orientation & low_spin)\n",
        "        terminal_reward = jnp.where(sl_nco_nls, 100.0 * jnp.exp(-delta_angle), terminal_reward)\n",
        "        nsl_cp = landed & ~slow & correct_position\n",
        "        terminal_reward = jnp.where(nsl_cp, params.reward_nsl_cp, terminal_reward)\n",
        "        nsl_ncp = landed & ~slow & ~correct_position\n",
        "        terminal_reward = jnp.where(nsl_ncp, params.reward_nsl_ncp, terminal_reward)\n",
        "        terminal_reward = jnp.where(escaped, params.reward_out_of_bounds, terminal_reward)\n",
        "        terminal_reward = jnp.where(timeout & ~landed & ~escaped, params.reward_timeout, terminal_reward)\n",
        "\n",
        "        ### Non-terminal (shaping) rewards\n",
        "        shaping_reward = jnp.array(0.0)\n",
        "\n",
        "        # Reward for firing engine while pointed retrograde (opposite to velocity direction)\n",
        "        # This encourages braking burns to slow down\n",
        "        thrust_angle = new_state.theta + new_state.gimbal\n",
        "        velocity_angle = jnp.arctan2(new_state.dy, new_state.dx)\n",
        "        alignment = jnp.cos(thrust_angle - velocity_angle)  # 1 = prograde, -1 = retrograde\n",
        "        retrograde_alignment = (1.0 - alignment) / 2.0  # 0 = prograde, 1 = retrograde\n",
        "        throttle_used = new_state.throttle > 0.1\n",
        "        velocity_factor = jnp.minimum(v_total / 10.0, 1.0)\n",
        "        retrograde_burn_reward = params.reward_retrograde_burn * retrograde_alignment * throttle_used * velocity_factor\n",
        "        shaping_reward += retrograde_burn_reward\n",
        "\n",
        "        # Reward for decreasing tangential velocity (we want v_tangential -> 0)\n",
        "        # Negative coefficient because we want to reward when (new - old) is negative\n",
        "        tangential_velocity_change = jnp.abs(v_tangential) - jnp.abs(v_tangential_old)\n",
        "        shaping_reward += -0.3 * tangential_velocity_change\n",
        "\n",
        "        # Reward for decreasing altitude, but only when near the target landing spot\n",
        "        # Weight by exp(-delta_angle) so we don't reward descending on the wrong side\n",
        "        delta_angle_factor = jnp.exp(-delta_angle * 3)\n",
        "        altitude_change = normalized_altitude - normalized_altitude_old\n",
        "        shaping_reward += -1.0 * delta_angle_factor * altitude_change\n",
        "\n",
        "        # Reward for improving orientation (theta_relative -> 0), but only when near landing\n",
        "        # near_landing_factor is high only when both altitude and velocity are low\n",
        "        near_landing_factor = jnp.exp(-normalized_altitude * 3.0) * jnp.exp(-v_total)\n",
        "        theta_relative_change = jnp.abs(theta_relative) - jnp.abs(theta_relative_old)\n",
        "        shaping_reward += -1.0 * near_landing_factor * theta_relative_change\n",
        "\n",
        "        # Reward for decreasing angular velocity (omega -> 0), but only when near landing\n",
        "        omega_change = jnp.abs(new_state.omega) - jnp.abs(old_state.omega)\n",
        "        shaping_reward += -0.1 * near_landing_factor * omega_change\n",
        "\n",
        "        # Bonus for being in the \"landing zone\" (low altitude AND near target)\n",
        "        # This is a constant bonus per timestep, not a change-based reward\n",
        "        low_altitude_factor = jnp.exp(-normalized_altitude * 2.0)\n",
        "        near_target_factor = jnp.exp(-delta_angle * 2.0)\n",
        "        landing_zone_bonus = 0.1 * low_altitude_factor * near_target_factor * near_target_factor\n",
        "        shaping_reward += landing_zone_bonus\n",
        "\n",
        "        # Reward for decreasing total velocity (we want v_total -> 0)\n",
        "        # Only reward decreases, don't punish increases (use max with 0)\n",
        "        velocity_change = v_total - v_total_old\n",
        "        shaping_reward += -0.1 * jnp.maximum(0.0, -velocity_change)  # reward when velocity_change < 0\n",
        "\n",
        "        # Punish high angular velocity (spinning too fast)\n",
        "        high_omega = jnp.abs(new_state.omega) > 2.0\n",
        "        angular_velocity_punishment = 0.1 * high_omega * (jnp.abs(new_state.omega) - 2.0)\n",
        "        shaping_reward -= angular_velocity_punishment\n",
        "\n",
        "        # Constant penalty for being far from target (angular distance)\n",
        "        # This discourages taking the \"long way around\" the planet\n",
        "        distance_from_target_penalty = 0.02 * delta_angle\n",
        "        shaping_reward -= distance_from_target_penalty\n",
        "\n",
        "        return jnp.where(is_terminal, terminal_reward, shaping_reward)\n",
        "\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return \"RocketLander\"\n",
        "\n",
        "    @property\n",
        "    def num_actions(self) -> int:\n",
        "        return 2\n",
        "\n",
        "    def action_space(self, params: Optional[EnvParams] = None) -> spaces.Box:\n",
        "        return spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=jnp.float32)\n",
        "\n",
        "    def observation_space(self, params: Optional[EnvParams] = None) -> spaces.Box:\n",
        "        low = jnp.array([-10.0, -jnp.pi, -50.0, -50.0, -jnp.pi, -10.0, 0.0, -0.5, 0.0])\n",
        "        high = jnp.array([150.0, jnp.pi, 50.0, 50.0, jnp.pi, 10.0, 1.0, 0.5, 5000.0])\n",
        "        return spaces.Box(low, high, shape=(9,), dtype=jnp.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualization Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_trajectory(states: list, params: EnvParams, title: str = \"Rocket Landing Trajectory\"):\n",
        "    \"\"\"Visualize the rocket trajectory around the planet.\"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "\n",
        "    # Draw planet\n",
        "    theta = np.linspace(0, 2 * np.pi, 100)\n",
        "    planet_x = params.planet_radius * np.cos(theta)\n",
        "    planet_y = params.planet_radius * np.sin(theta)\n",
        "    ax.fill(planet_x, planet_y, color='#3d5a80', alpha=0.8, label='Planet')\n",
        "    ax.plot(planet_x, planet_y, color='#1d3557', linewidth=2)\n",
        "\n",
        "    # Extract trajectory\n",
        "    xs = np.array([float(s.x) for s in states])\n",
        "    ys = np.array([float(s.y) for s in states])\n",
        "    throttles = np.array([float(s.throttle) for s in states])\n",
        "\n",
        "    # Draw trajectory colored by throttle\n",
        "    points = np.array([xs, ys]).T.reshape(-1, 1, 2)\n",
        "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "\n",
        "    norm = plt.Normalize(0, 1)\n",
        "    lc = LineCollection(segments, cmap='YlOrRd', norm=norm, linewidth=2, alpha=0.8)\n",
        "    lc.set_array(throttles[:-1])\n",
        "    ax.add_collection(lc)\n",
        "    cbar = plt.colorbar(lc, ax=ax, label='Throttle')\n",
        "    cbar.ax.yaxis.set_tick_params(color='white')\n",
        "    cbar.ax.yaxis.label.set_color('white')\n",
        "    plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color='white')\n",
        "\n",
        "    # Mark start and end\n",
        "    ax.scatter(xs[0], ys[0], s=100, c='green', marker='o', zorder=5, label='Start')\n",
        "    ax.scatter(xs[-1], ys[-1], s=100, c='red', marker='x', zorder=5, label='End')\n",
        "\n",
        "    # Draw target landing site\n",
        "    target_angle = float(states[0].target_angle)\n",
        "    target_x = params.planet_radius * np.cos(target_angle)\n",
        "    target_y = params.planet_radius * np.sin(target_angle)\n",
        "    ax.scatter(target_x, target_y, s=200, c='yellow', marker='*', zorder=5, edgecolors='black', label='Target')\n",
        "\n",
        "    # Draw rocket orientation arrows\n",
        "    n_arrows = min(30, len(states))\n",
        "    arrow_indices = np.linspace(0, len(states) - 1, n_arrows, dtype=int)\n",
        "    for idx in arrow_indices:\n",
        "        s = states[idx]\n",
        "        x, y = float(s.x), float(s.y)\n",
        "        th = float(s.theta)\n",
        "        dx = np.sin(th) * 8\n",
        "        dy = np.cos(th) * 8\n",
        "        ax.arrow(x, y, dx, dy, head_width=4, head_length=2, fc='#cccccc', ec='white', alpha=0.9)\n",
        "\n",
        "    # Set view\n",
        "    fov = params.init_max_orbit_radius * 1.2\n",
        "    ax.set_xlim(-fov, fov)\n",
        "    ax.set_ylim(-fov, fov)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlabel('X Position')\n",
        "    ax.set_ylabel('Y Position')\n",
        "    ax.set_title(title)\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Dark background\n",
        "    ax.set_facecolor('#0d1b2a')\n",
        "    fig.patch.set_facecolor('#0d1b2a')\n",
        "    ax.tick_params(colors='white')\n",
        "    ax.xaxis.label.set_color('white')\n",
        "    ax.yaxis.label.set_color('white')\n",
        "    ax.title.set_color('white')\n",
        "    ax.legend(facecolor='#1b263b', edgecolor='white', labelcolor='white')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Neural Network Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_mlp(layer_sizes, key: PRNGKey, scale: float = 1e-2):\n",
        "    \"\"\"Initialize MLP parameters.\"\"\"\n",
        "    keys = jr.split(key, 2 * len(layer_sizes))\n",
        "    params = []\n",
        "    for i in range(len(layer_sizes) - 1):\n",
        "        input_size, output_size = layer_sizes[i], layer_sizes[i + 1]\n",
        "        W = jr.normal(keys[2 * i], (input_size, output_size)) * scale\n",
        "        b = jr.normal(keys[2 * i + 1], (output_size,)) * scale\n",
        "        params.append((W, b))\n",
        "    return params\n",
        "\n",
        "\n",
        "def policy(params, x):\n",
        "    \"\"\"Gaussian policy network.\"\"\"\n",
        "    for (W, b) in params[:-1]:\n",
        "        x = jnp.dot(x, W) + b\n",
        "        x = jax.nn.relu(x)\n",
        "\n",
        "    W, b = params[-1]\n",
        "    output = jnp.dot(x, W) + b\n",
        "    \n",
        "    action_dim = output.shape[-1] // 2\n",
        "    mean = jnp.tanh(output[:action_dim])\n",
        "    log_std = jnp.clip(output[action_dim:], -2.0, 0.5)\n",
        "    \n",
        "    return mean, log_std\n",
        "\n",
        "\n",
        "def get_action(params, x, key: PRNGKey):\n",
        "    \"\"\"Sample an action from the Gaussian policy.\"\"\"\n",
        "    mean, log_std = policy(params, x)\n",
        "    std = jnp.exp(log_std)\n",
        "    noise = jr.normal(key, shape=mean.shape)\n",
        "    action = jnp.clip(mean + std * noise, -1.0, 1.0)\n",
        "    return action, mean, log_std\n",
        "\n",
        "\n",
        "def get_log_prob(params, x, action):\n",
        "    \"\"\"Compute log probability of action under the Gaussian policy.\"\"\"\n",
        "    mean, log_std = policy(params, x)\n",
        "    std = jnp.exp(log_std)\n",
        "    log_prob = -0.5 * jnp.sum(((action - mean) / std) ** 2 + 2 * log_std + jnp.log(2 * jnp.pi))\n",
        "    return log_prob\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def update_delta(delta, grad_theta):\n",
        "    \"\"\"Update the parameter gradients.\"\"\"\n",
        "    return jax.tree.map(lambda x, y: x + y, delta, grad_theta), None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Rocket Landing with Reinforcement Learning\n",
        "\n",
        "This notebook trains a neural network policy to land a rocket on a planet using REINFORCE.\n",
        "\n",
        "**Features:**\n",
        "- 2D physics simulation with gravity\n",
        "- Rocket starts in orbit, must land at target location\n",
        "- Continuous control: throttle and gimbal angle\n",
        "- GPU-accelerated training with JAX\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's check if we have GPU access and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q jax[cuda12] flax optax gymnax matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify JAX GPU setup\n",
        "import jax\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Backend: {jax.default_backend()}\")\n",
        "print(f\"Devices: {jax.devices()}\")\n",
        "\n",
        "if jax.default_backend() == 'gpu':\n",
        "    print(\"\\nâœ… GPU acceleration enabled!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Running on CPU. Go to Runtime > Change runtime type > GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Definition\n",
        "\n",
        "The rocket environment simulates:\n",
        "- A planet with gravitational pull\n",
        "- A rocket that starts in orbit\n",
        "- Target landing location on the planet surface\n",
        "- Fuel consumption and thrust physics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"JAX implementation of rocket landing environment.\"\"\"\n",
        "from typing import Any, Optional, Tuple\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import struct\n",
        "from gymnax.environments import environment, spaces\n",
        "\n",
        "# GRAVITATIONAL_CONSTANT\n",
        "G = 6.67430e-11\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvState(environment.EnvState):\n",
        "    \"\"\"State of the rocket landing environment.\"\"\"\n",
        "    x: jnp.ndarray\n",
        "    y: jnp.ndarray\n",
        "    theta: jnp.ndarray\n",
        "    dx: jnp.ndarray\n",
        "    dy: jnp.ndarray\n",
        "    omega: jnp.ndarray\n",
        "    throttle: jnp.ndarray\n",
        "    gimbal: jnp.ndarray\n",
        "    fuel: jnp.ndarray\n",
        "    target_angle: jnp.ndarray\n",
        "    time: jnp.ndarray\n",
        "\n",
        "@struct.dataclass\n",
        "class EnvParams(environment.EnvParams):\n",
        "    \"\"\"Parameters for the rocket landing environment.\"\"\"\n",
        "    dt: float = 0.05\n",
        "    planet_radius: float = 50.\n",
        "    planet_mass: float = 5.0e14\n",
        "    rocket_max_thrust: float = 38000.0\n",
        "    rocket_max_gimbal: float = 0.3\n",
        "    rocket_mass: float = 2600.0\n",
        "    rocket_moment_of_inertia: float = 250.0\n",
        "    rocket_initial_fuel: float = 5000.0\n",
        "    rocket_fuel_consumption_rate: float = 100.\n",
        "    init_min_orbit_radius: float = 80.0\n",
        "    init_max_orbit_radius: float = 150.0\n",
        "    init_orbit_velocity_noise: float = 0.1\n",
        "    max_steps_in_episode: int = 1500\n",
        "    landing_max_speed: float = 2.0\n",
        "    landing_max_angle: float = 0.3\n",
        "    landing_max_omega: float = 0.5\n",
        "    landing_position_tolerance: float = jnp.pi/30\n",
        "    # Terminal rewards\n",
        "    reward_sl_cp_co_ls: float = 1000.0\n",
        "    reward_nsl_cp: float = 200.0\n",
        "    reward_nsl_ncp: float = 100.0\n",
        "    reward_sl_ncp_co_ls: float = 500.0\n",
        "    reward_out_of_bounds: float = -1000.0\n",
        "    reward_timeout: float = -2000.0\n",
        "    reward_retrograde_burn: float = 0.02\n",
        "\n",
        "def angle_normalize(x):\n",
        "    \"\"\"Normalize the angle - radians.\"\"\"\n",
        "    return ((x + jnp.pi) % (2 * jnp.pi)) - jnp.pi\n",
        "\n",
        "class RocketLander(environment.Environment[EnvState, EnvParams]):\n",
        "    \"\"\"JAX/Gymnax implementation of a 2D rocket landing environment.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @property\n",
        "    def default_params(self) -> EnvParams:\n",
        "        return EnvParams()\n",
        "\n",
        "    def step_env(self, key, state, action, params):\n",
        "        \"\"\"Integrate physics for one timestep.\"\"\"\n",
        "        r = jnp.sqrt(state.x**2 + state.y**2)\n",
        "        a_gravity_magnitude = G * params.planet_mass / r**2\n",
        "        a_g_x = -a_gravity_magnitude * state.x / r\n",
        "        a_g_y = -a_gravity_magnitude * state.y / r\n",
        "\n",
        "        throttle = jnp.clip(action[0], -1.0, 1.0)\n",
        "        gimbal = jnp.clip(action[1], -1.0, 1.0)\n",
        "        throttle = (throttle + 1.0) / 2.0\n",
        "        gimbal *= params.rocket_max_gimbal\n",
        "        throttle = jnp.minimum(throttle, state.fuel / (params.rocket_fuel_consumption_rate * params.dt))\n",
        "\n",
        "        thrust_angle = state.theta + gimbal\n",
        "        F_thrust = throttle * params.rocket_max_thrust\n",
        "        a_t_x = F_thrust * jnp.sin(thrust_angle) / params.rocket_mass\n",
        "        a_t_y = F_thrust * jnp.cos(thrust_angle) / params.rocket_mass\n",
        "        a_omega = F_thrust * jnp.sin(gimbal) / params.rocket_moment_of_inertia\n",
        "\n",
        "        a_x = a_g_x + a_t_x\n",
        "        a_y = a_g_y + a_t_y\n",
        "        dx = state.dx + a_x * params.dt\n",
        "        dy = state.dy + a_y * params.dt\n",
        "        omega = state.omega + a_omega * params.dt\n",
        "        x = state.x + dx * params.dt\n",
        "        y = state.y + dy * params.dt\n",
        "        theta = state.theta + omega * params.dt\n",
        "        fuel = jnp.maximum(0.0, state.fuel - throttle * params.rocket_fuel_consumption_rate * params.dt)\n",
        "\n",
        "        new_state = EnvState(\n",
        "            x=x, y=y, theta=theta, dx=dx, dy=dy, omega=omega,\n",
        "            throttle=throttle, gimbal=gimbal, fuel=fuel,\n",
        "            target_angle=state.target_angle, time=state.time + params.dt\n",
        "        )\n",
        "        done = self.is_terminal(new_state, params)\n",
        "        reward = self._compute_reward(state, new_state, params)\n",
        "        key, noise_key = jax.random.split(key)\n",
        "        obs = self.get_obs(new_state, params, noise_key)\n",
        "        return obs, new_state, reward, done, {}\n",
        "\n",
        "    def reset_env(self, key, params):\n",
        "        \"\"\"Reset environment by sampling initial orbit.\"\"\"\n",
        "        key, angle_key, orbit_radius_key, orbit_vel_noise_key, target_angle_key = jax.random.split(key, 5)\n",
        "        angle_init = jax.random.uniform(angle_key, minval=0.0, maxval=2.0 * jnp.pi)\n",
        "        orbit_radius_init = jax.random.uniform(orbit_radius_key, minval=params.init_min_orbit_radius, maxval=params.init_max_orbit_radius)\n",
        "        x_init = orbit_radius_init * jnp.cos(angle_init)\n",
        "        y_init = orbit_radius_init * jnp.sin(angle_init)\n",
        "        orbit_velocity_magnitude_init = jnp.sqrt(G * params.planet_mass / orbit_radius_init)\n",
        "        velocity_noise = params.init_orbit_velocity_noise * (2.0 * jax.random.uniform(orbit_vel_noise_key) - 1.0)\n",
        "        orbit_velocity_magnitude_init *= (1.0 + velocity_noise)\n",
        "        dx_init = -orbit_velocity_magnitude_init * jnp.sin(angle_init)\n",
        "        dy_init = orbit_velocity_magnitude_init * jnp.cos(angle_init)\n",
        "        theta_init = angle_init + (3/2) * jnp.pi\n",
        "        target_angle = jax.random.uniform(target_angle_key, minval=0.0, maxval=2.0 * jnp.pi)\n",
        "\n",
        "        state = EnvState(\n",
        "            x=x_init, y=y_init, theta=theta_init, dx=dx_init, dy=dy_init,\n",
        "            omega=jnp.array(0.0), throttle=jnp.array(0.0), gimbal=jnp.array(0.0),\n",
        "            fuel=jnp.array(params.rocket_initial_fuel), target_angle=target_angle, time=jnp.array(0)\n",
        "        )\n",
        "        return self.get_obs(state, params, key), state\n",
        "\n",
        "    def get_obs(self, state, params=None, key=None):\n",
        "        \"\"\"Get observation vector.\"\"\"\n",
        "        if params is None:\n",
        "            params = self.default_params\n",
        "        r = jnp.sqrt(state.x ** 2 + state.y ** 2)\n",
        "        altitude = r - params.planet_radius\n",
        "        positional_angle = jnp.arctan2(state.y, state.x)\n",
        "        delta_angle = angle_normalize(state.target_angle - positional_angle)\n",
        "        radial_vel = (state.x * state.dx + state.y * state.dy) / r\n",
        "        tangential_vel = (state.x * state.dy - state.y * state.dx) / r\n",
        "        theta_relative = angle_normalize(state.theta - positional_angle)\n",
        "        return jnp.array([altitude, delta_angle, radial_vel, tangential_vel, theta_relative, state.omega, state.throttle, state.gimbal, state.fuel])\n",
        "\n",
        "    def is_terminal(self, state, params):\n",
        "        \"\"\"Check whether state is terminal.\"\"\"\n",
        "        max_time = params.max_steps_in_episode * params.dt\n",
        "        timeout = state.time >= max_time\n",
        "        r = jnp.sqrt(state.x ** 2 + state.y ** 2)\n",
        "        landed = r <= params.planet_radius\n",
        "        escaped = r > params.planet_radius * 4\n",
        "        return jnp.array(landed | timeout | escaped)\n",
        "\n",
        "    def _compute_reward(self, old_state, new_state, params):\n",
        "        \"\"\"Compute reward with terminal rewards REPLACING shaping rewards.\"\"\"\n",
        "        r = jnp.sqrt(new_state.x ** 2 + new_state.y ** 2)\n",
        "        altitude = r - params.planet_radius\n",
        "        normalized_altitude = jnp.clip(altitude / (params.init_max_orbit_radius - params.planet_radius), 0.0, 2.0)\n",
        "        landed = r <= params.planet_radius\n",
        "        escaped = r > params.planet_radius * 4\n",
        "        max_time = params.max_steps_in_episode * params.dt\n",
        "        timeout = new_state.time >= max_time\n",
        "        is_terminal = landed | escaped | timeout\n",
        "\n",
        "        positional_angle = jnp.arctan2(new_state.y, new_state.x)\n",
        "        delta_angle = jnp.abs(angle_normalize(new_state.target_angle - positional_angle))\n",
        "        v_total = jnp.sqrt(new_state.dx ** 2 + new_state.dy ** 2)\n",
        "        v_tangential = (new_state.x * new_state.dy - new_state.y * new_state.dx) / r\n",
        "        theta_relative = angle_normalize(new_state.theta - positional_angle)\n",
        "\n",
        "        r_old = jnp.sqrt(old_state.x ** 2 + old_state.y ** 2)\n",
        "        normalized_altitude_old = jnp.clip((r_old - params.planet_radius) / (params.init_max_orbit_radius - params.planet_radius), 0.0, 2.0)\n",
        "        positional_angle_old = jnp.arctan2(old_state.y, old_state.x)\n",
        "        delta_angle_old = jnp.abs(angle_normalize(old_state.target_angle - positional_angle_old))\n",
        "        v_total_old = jnp.sqrt(old_state.dx ** 2 + old_state.dy ** 2)\n",
        "        v_tangential_old = (old_state.x * old_state.dy - old_state.y * old_state.dx) / r_old\n",
        "        theta_relative_old = jnp.abs(angle_normalize(old_state.theta - positional_angle_old))\n",
        "\n",
        "        slow = v_total < params.landing_max_speed\n",
        "        correct_position = delta_angle <= params.landing_position_tolerance\n",
        "        correct_orientation = jnp.abs(theta_relative) <= params.landing_max_angle\n",
        "        low_spin = jnp.abs(new_state.omega) <= params.landing_max_omega\n",
        "\n",
        "        # Terminal rewards\n",
        "        terminal_reward = jnp.array(0.0)\n",
        "        sl_cp_co_ls = landed & slow & correct_position & correct_orientation & low_spin\n",
        "        terminal_reward = jnp.where(sl_cp_co_ls, params.reward_sl_cp_co_ls, terminal_reward)\n",
        "        sl_ncp_co_ls = landed & slow & ~correct_position & correct_orientation & low_spin\n",
        "        terminal_reward = jnp.where(sl_ncp_co_ls, params.reward_sl_ncp_co_ls * jnp.exp(-delta_angle * 2.0), terminal_reward)\n",
        "        sl_nco_nls = landed & slow & ~(correct_orientation & low_spin)\n",
        "        terminal_reward = jnp.where(sl_nco_nls, 100.0 * jnp.exp(-delta_angle), terminal_reward)\n",
        "        nsl_cp = landed & ~slow & correct_position\n",
        "        terminal_reward = jnp.where(nsl_cp, params.reward_nsl_cp, terminal_reward)\n",
        "        nsl_ncp = landed & ~slow & ~correct_position\n",
        "        terminal_reward = jnp.where(nsl_ncp, params.reward_nsl_ncp, terminal_reward)\n",
        "        terminal_reward = jnp.where(escaped, params.reward_out_of_bounds, terminal_reward)\n",
        "        terminal_reward = jnp.where(timeout & ~landed & ~escaped, params.reward_timeout, terminal_reward)\n",
        "\n",
        "        ### Non-terminal (shaping) rewards\n",
        "        shaping_reward = jnp.array(0.0)\n",
        "\n",
        "        # Reward for firing engine while pointed retrograde (opposite to velocity direction)\n",
        "        # This encourages braking burns to slow down\n",
        "        thrust_angle = new_state.theta + new_state.gimbal\n",
        "        velocity_angle = jnp.arctan2(new_state.dy, new_state.dx)\n",
        "        alignment = jnp.cos(thrust_angle - velocity_angle)  # 1 = prograde, -1 = retrograde\n",
        "        retrograde_alignment = (1.0 - alignment) / 2.0  # 0 = prograde, 1 = retrograde\n",
        "        throttle_used = new_state.throttle > 0.1\n",
        "        velocity_factor = jnp.minimum(v_total / 10.0, 1.0)\n",
        "        retrograde_burn_reward = params.reward_retrograde_burn * retrograde_alignment * throttle_used * velocity_factor\n",
        "        shaping_reward += retrograde_burn_reward\n",
        "\n",
        "        # Reward for decreasing tangential velocity (we want v_tangential -> 0)\n",
        "        # Negative coefficient because we want to reward when (new - old) is negative\n",
        "        tangential_velocity_change = jnp.abs(v_tangential) - jnp.abs(v_tangential_old)\n",
        "        shaping_reward += -0.3 * tangential_velocity_change\n",
        "\n",
        "        # Reward for decreasing altitude, but only when near the target landing spot\n",
        "        # Weight by exp(-delta_angle) so we don't reward descending on the wrong side\n",
        "        delta_angle_factor = jnp.exp(-delta_angle * 3)\n",
        "        altitude_change = normalized_altitude - normalized_altitude_old\n",
        "        shaping_reward += -1.0 * delta_angle_factor * altitude_change\n",
        "\n",
        "        # Reward for improving orientation (theta_relative -> 0), but only when near landing\n",
        "        # near_landing_factor is high only when both altitude and velocity are low\n",
        "        near_landing_factor = jnp.exp(-normalized_altitude * 3.0) * jnp.exp(-v_total)\n",
        "        theta_relative_change = jnp.abs(theta_relative) - jnp.abs(theta_relative_old)\n",
        "        shaping_reward += -1.0 * near_landing_factor * theta_relative_change\n",
        "\n",
        "        # Reward for decreasing angular velocity (omega -> 0), but only when near landing\n",
        "        omega_change = jnp.abs(new_state.omega) - jnp.abs(old_state.omega)\n",
        "        shaping_reward += -0.1 * near_landing_factor * omega_change\n",
        "\n",
        "        # Bonus for being in the \"landing zone\" (low altitude AND near target)\n",
        "        # This is a constant bonus per timestep, not a change-based reward\n",
        "        low_altitude_factor = jnp.exp(-normalized_altitude * 2.0)\n",
        "        near_target_factor = jnp.exp(-delta_angle * 2.0)\n",
        "        landing_zone_bonus = 0.1 * low_altitude_factor * near_target_factor * near_target_factor\n",
        "        shaping_reward += landing_zone_bonus\n",
        "\n",
        "        # Reward for decreasing total velocity (we want v_total -> 0)\n",
        "        # Only reward decreases, don't punish increases (use max with 0)\n",
        "        velocity_change = v_total - v_total_old\n",
        "        shaping_reward += -0.1 * jnp.maximum(0.0, -velocity_change)  # reward when velocity_change < 0\n",
        "\n",
        "        # Punish high angular velocity (spinning too fast)\n",
        "        high_omega = jnp.abs(new_state.omega) > 2.0\n",
        "        angular_velocity_punishment = 0.1 * high_omega * (jnp.abs(new_state.omega) - 2.0)\n",
        "        shaping_reward -= angular_velocity_punishment\n",
        "\n",
        "        # Constant penalty for being far from target (angular distance)\n",
        "        # This discourages taking the \"long way around\" the planet\n",
        "        distance_from_target_penalty = 0.02 * delta_angle\n",
        "        shaping_reward -= distance_from_target_penalty\n",
        "\n",
        "        return jnp.where(is_terminal, terminal_reward, shaping_reward)\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return \"RocketLander\"\n",
        "\n",
        "    @property\n",
        "    def num_actions(self):\n",
        "        return 2\n",
        "\n",
        "    def action_space(self, params=None):\n",
        "        return spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=jnp.float32)\n",
        "\n",
        "    def observation_space(self, params=None):\n",
        "        low = jnp.array([-10.0, -jnp.pi, -50.0, -50.0, -jnp.pi, -10.0, 0.0, -0.5, 0.0])\n",
        "        high = jnp.array([150.0, jnp.pi, 50.0, 50.0, jnp.pi, 10.0, 1.0, 0.5, 5000.0])\n",
        "        return spaces.Box(low, high, shape=(9,), dtype=jnp.float32)\n",
        "\n",
        "print(\"âœ… Environment defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Visualization utilities.\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "def visualize_trajectory(states, params, title=\"Rocket Landing Trajectory\", save_path=None):\n",
        "    \"\"\"Visualize the rocket trajectory around the planet.\"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "\n",
        "    # Draw planet\n",
        "    theta = np.linspace(0, 2 * np.pi, 100)\n",
        "    planet_x = params.planet_radius * np.cos(theta)\n",
        "    planet_y = params.planet_radius * np.sin(theta)\n",
        "    ax.fill(planet_x, planet_y, color='#3d5a80', alpha=0.8, label='Planet')\n",
        "    ax.plot(planet_x, planet_y, color='#1d3557', linewidth=2)\n",
        "\n",
        "    # Extract trajectory\n",
        "    xs = np.array([float(s.x) for s in states])\n",
        "    ys = np.array([float(s.y) for s in states])\n",
        "    throttles = np.array([float(s.throttle) for s in states])\n",
        "\n",
        "    # Draw trajectory colored by throttle\n",
        "    points = np.array([xs, ys]).T.reshape(-1, 1, 2)\n",
        "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "    norm = plt.Normalize(0, 1)\n",
        "    lc = LineCollection(segments, cmap='YlOrRd', norm=norm, linewidth=2, alpha=0.8)\n",
        "    lc.set_array(throttles[:-1])\n",
        "    ax.add_collection(lc)\n",
        "    cbar = plt.colorbar(lc, ax=ax, label='Throttle')\n",
        "    cbar.ax.yaxis.set_tick_params(color='white')\n",
        "    cbar.ax.yaxis.label.set_color('white')\n",
        "    plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color='white')\n",
        "\n",
        "    # Markers\n",
        "    ax.scatter(xs[0], ys[0], s=100, c='green', marker='o', zorder=5, label='Start')\n",
        "    ax.scatter(xs[-1], ys[-1], s=100, c='red', marker='x', zorder=5, label='End')\n",
        "\n",
        "    # Target\n",
        "    target_angle = float(states[0].target_angle)\n",
        "    target_x = params.planet_radius * np.cos(target_angle)\n",
        "    target_y = params.planet_radius * np.sin(target_angle)\n",
        "    ax.scatter(target_x, target_y, s=200, c='yellow', marker='*', zorder=5, edgecolors='black', label='Target')\n",
        "\n",
        "    # Rocket orientation arrows\n",
        "    n_arrows = min(30, len(states))\n",
        "    arrow_indices = np.linspace(0, len(states) - 1, n_arrows, dtype=int)\n",
        "    for idx in arrow_indices:\n",
        "        s = states[idx]\n",
        "        x, y = float(s.x), float(s.y)\n",
        "        theta = float(s.theta)\n",
        "        dx = np.sin(theta) * 8\n",
        "        dy = np.cos(theta) * 8\n",
        "        ax.arrow(x, y, dx, dy, head_width=4, head_length=2, fc='#cccccc', ec='white', alpha=0.9)\n",
        "\n",
        "    # Styling\n",
        "    fov = params.init_max_orbit_radius * 1.2\n",
        "    ax.set_xlim(-fov, fov)\n",
        "    ax.set_ylim(-fov, fov)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlabel('X Position')\n",
        "    ax.set_ylabel('Y Position')\n",
        "    ax.set_title(title)\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_facecolor('#0d1b2a')\n",
        "    fig.patch.set_facecolor('#0d1b2a')\n",
        "    ax.tick_params(colors='white')\n",
        "    ax.xaxis.label.set_color('white')\n",
        "    ax.yaxis.label.set_color('white')\n",
        "    ax.title.set_color('white')\n",
        "    ax.legend(facecolor='#1b263b', edgecolor='white', labelcolor='white')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, facecolor=fig.get_facecolor())\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "    return fig\n",
        "\n",
        "print(\"âœ… Visualization utilities defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Network Policy and Training\n",
        "\n",
        "We use REINFORCE with a Gaussian policy to train the rocket landing agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Training components.\"\"\"\n",
        "import jax.random as jr\n",
        "from jax.random import PRNGKey\n",
        "import optax\n",
        "\n",
        "# Create the environment\n",
        "env = RocketLander()\n",
        "env_params = env.default_params\n",
        "\n",
        "def initialize_mlp(layer_sizes, key, scale=1e-2):\n",
        "    \"\"\"Initialize MLP parameters.\"\"\"\n",
        "    keys = jr.split(key, 2 * len(layer_sizes))\n",
        "    params = []\n",
        "    for i in range(len(layer_sizes) - 1):\n",
        "        input_size, output_size = layer_sizes[i], layer_sizes[i + 1]\n",
        "        W = jr.normal(keys[2 * i], (input_size, output_size)) * scale\n",
        "        b = jr.normal(keys[2 * i + 1], (output_size,)) * scale\n",
        "        params.append((W, b))\n",
        "    return params\n",
        "\n",
        "def policy(params, x):\n",
        "    \"\"\"Gaussian policy network.\"\"\"\n",
        "    for (W, b) in params[:-1]:\n",
        "        x = jnp.dot(x, W) + b\n",
        "        x = jax.nn.relu(x)\n",
        "    W, b = params[-1]\n",
        "    output = jnp.dot(x, W) + b\n",
        "    action_dim = output.shape[-1] // 2\n",
        "    mean = jnp.tanh(output[:action_dim])\n",
        "    log_std = jnp.clip(output[action_dim:], -2.0, 0.5)\n",
        "    return mean, log_std\n",
        "\n",
        "def get_action(params, x, key):\n",
        "    \"\"\"Sample an action from the Gaussian policy.\"\"\"\n",
        "    mean, log_std = policy(params, x)\n",
        "    std = jnp.exp(log_std)\n",
        "    noise = jr.normal(key, shape=mean.shape)\n",
        "    action = jnp.clip(mean + std * noise, -1.0, 1.0)\n",
        "    return action, mean, log_std\n",
        "\n",
        "def get_log_prob(params, x, action):\n",
        "    \"\"\"Compute log probability of action under the Gaussian policy.\"\"\"\n",
        "    mean, log_std = policy(params, x)\n",
        "    std = jnp.exp(log_std)\n",
        "    return -0.5 * jnp.sum(((action - mean) / std) ** 2 + 2 * log_std + jnp.log(2 * jnp.pi))\n",
        "\n",
        "@jax.jit\n",
        "def update_delta(delta, grad_theta):\n",
        "    \"\"\"Update the parameter gradients.\"\"\"\n",
        "    return jax.tree.map(lambda x, y: x + y, delta, grad_theta), None\n",
        "\n",
        "def rollout(params, env_params, rng_input, steps_in_episode):\n",
        "    \"\"\"Rollout an episode using the policy.\"\"\"\n",
        "    rng_reset, rng_episode = jr.split(rng_input)\n",
        "    obs, state = env.reset_env(rng_reset, env_params)\n",
        "\n",
        "    def policy_step(state_input, tmp):\n",
        "        obs, state, rng = state_input\n",
        "        rng, rng_action, rng_step = jr.split(rng, 3)\n",
        "        action, mean, log_std = get_action(params, obs, rng_action)\n",
        "        next_obs, next_state, reward, done, _ = env.step_env(rng_step, state, action, env_params)\n",
        "        carry = [next_obs, next_state, rng]\n",
        "        return carry, [obs, state, action, reward, next_obs, done]\n",
        "\n",
        "    _, scan_out = jax.lax.scan(policy_step, [obs, state, rng_episode], (), length=steps_in_episode)\n",
        "    return scan_out\n",
        "\n",
        "jit_rollout = jax.jit(rollout, static_argnums=3)\n",
        "\n",
        "def loss_REINFORCE(params, obs, action, reward, baseline, gamma=0.99):\n",
        "    \"\"\"Compute REINFORCE loss with baseline.\"\"\"\n",
        "    def trajectory_gradients(reward, obs, action, baseline, delta):\n",
        "        G_init = 0.0\n",
        "        def step(carry, variables):\n",
        "            G, delta = carry\n",
        "            r, obs, action, b = variables\n",
        "            G = gamma * G + r\n",
        "            advantage = G - b\n",
        "            def neg_log_prob(params):\n",
        "                return -get_log_prob(params, obs, action)\n",
        "            grad_delta = jax.grad(neg_log_prob)(params)\n",
        "            grad_delta = jax.tree.map(lambda gd: gd * advantage, grad_delta)\n",
        "            delta, _ = update_delta(delta, grad_delta)\n",
        "            return (G, delta), G\n",
        "        variables = (reward[::-1], obs[::-1], action[::-1], baseline[::-1])\n",
        "        (_, delta), Gt = jax.lax.scan(step, (G_init, delta), variables)\n",
        "        return delta, Gt\n",
        "\n",
        "    parallel_trajectory_gradients = jax.vmap(trajectory_gradients, in_axes=(0, 0, 0, None, None))\n",
        "    delta = jax.tree.map(lambda t: jnp.zeros(t.shape), params)\n",
        "    deltas, Gs = parallel_trajectory_gradients(reward, obs, action, baseline, delta)\n",
        "    delta, _ = jax.lax.scan(update_delta, delta, deltas)\n",
        "    return delta, jnp.array(Gs)\n",
        "\n",
        "loss_REINFORCE = jax.jit(loss_REINFORCE)\n",
        "\n",
        "def mean_baseline(Gs):\n",
        "    \"\"\"Compute constant baseline as mean of discounted returns.\"\"\"\n",
        "    T = Gs.shape[1]\n",
        "    return jnp.ones((T,)) * jnp.mean(Gs)\n",
        "\n",
        "print(\"âœ… Training components defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    num_iters=5000,\n",
        "    steps_in_episode=1500,\n",
        "    lr=0.0002,\n",
        "    gamma=0.995,\n",
        "    n_batches=128,\n",
        "    hidden_size=256,\n",
        "    n_hidden_layers=3,\n",
        "    visualize_every=500,\n",
        "    seed=42,\n",
        "    grad_clip=0.5,\n",
        "):\n",
        "    \"\"\"Train the rocket landing policy using REINFORCE.\"\"\"\n",
        "    key = PRNGKey(seed)\n",
        "    \n",
        "    # Network architecture\n",
        "    obs_dim = 9\n",
        "    action_dim = 2\n",
        "    hidden_layers = tuple([hidden_size] * n_hidden_layers)\n",
        "    layer_sizes = (obs_dim,) + hidden_layers + (2 * action_dim,)\n",
        "    \n",
        "    # Initialize\n",
        "    key, subkey = jr.split(key)\n",
        "    params = initialize_mlp(layer_sizes, key=subkey)\n",
        "    optim = optax.chain(optax.clip_by_global_norm(grad_clip), optax.adam(learning_rate=lr))\n",
        "    opt_state = optim.init(params)\n",
        "    Gs = jnp.zeros((n_batches, steps_in_episode))\n",
        "    \n",
        "    key, subkey = jr.split(key)\n",
        "    iter_keys = jr.split(subkey, num_iters)\n",
        "    parallel_rollout = jax.vmap(rollout, in_axes=(None, None, 0, None))\n",
        "    \n",
        "    def step(carry, key):\n",
        "        params, opt_state, Gs = carry\n",
        "        keys = jr.split(key, n_batches)\n",
        "        obs, state, action, reward, next_obs, done = parallel_rollout(params, env_params, keys, steps_in_episode)\n",
        "        baseline = mean_baseline(Gs)\n",
        "        delta, Gs_new = loss_REINFORCE(params, obs, action, reward, baseline, gamma)\n",
        "        updates, opt_state = optim.update(delta, opt_state, params)\n",
        "        new_params = optax.apply_updates(params, updates)\n",
        "        mean_reward = jnp.mean(jnp.sum(reward, axis=-1))\n",
        "        return (new_params, opt_state, Gs_new), mean_reward\n",
        "\n",
        "    step = jax.jit(step)\n",
        "    \n",
        "    history = []\n",
        "    current_params = params\n",
        "    current_opt_state = opt_state\n",
        "    current_Gs = Gs\n",
        "    \n",
        "    print(f\"ðŸš€ Starting training for {num_iters} iterations...\")\n",
        "    print(f\"   Network: {layer_sizes}\")\n",
        "    print(f\"   Batch size: {n_batches}, Steps: {steps_in_episode}\")\n",
        "    print(f\"   LR: {lr}, Gamma: {gamma}\")\n",
        "    print()\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "        (current_params, current_opt_state, current_Gs), mean_reward = step(\n",
        "            (current_params, current_opt_state, current_Gs), iter_keys[i]\n",
        "        )\n",
        "        history.append(float(mean_reward))\n",
        "        \n",
        "        if (i + 1) % 100 == 0:\n",
        "            avg_reward = jnp.mean(jnp.array(history[-100:]))\n",
        "            print(f\"Iteration {i + 1}/{num_iters} | Avg Reward (last 100): {avg_reward:.2f}\")\n",
        "        \n",
        "        if (i + 1) % visualize_every == 0:\n",
        "            print(f\"\\nðŸ“Š Visualizing at iteration {i + 1}...\")\n",
        "            key, vis_key = jr.split(key)\n",
        "            visualize_policy(current_params, vis_key, title=f\"Iteration {i + 1}\")\n",
        "    \n",
        "    print(\"\\nâœ… Training complete!\")\n",
        "    key, vis_key = jr.split(key)\n",
        "    visualize_policy(current_params, vis_key, title=\"Final Policy\")\n",
        "    \n",
        "    # Plot training curve\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(history, alpha=0.3, label='Episode Reward')\n",
        "    window = min(100, len(history) // 10)\n",
        "    if window > 1:\n",
        "        smoothed = jnp.convolve(jnp.array(history), jnp.ones(window) / window, mode='valid')\n",
        "        plt.plot(range(window - 1, len(history)), smoothed, label=f'Smoothed (window={window})')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Mean Episode Reward')\n",
        "    plt.title('Training Progress')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return current_params, history\n",
        "\n",
        "\n",
        "def visualize_policy(params, key, title=\"Policy Trajectory\"):\n",
        "    \"\"\"Run a single episode and visualize the trajectory.\"\"\"\n",
        "    obs, state, action, reward, next_obs, done = rollout(params, env_params, key, env_params.max_steps_in_episode)\n",
        "    states = []\n",
        "    for i in range(len(state.x)):\n",
        "        s = EnvState(\n",
        "            x=state.x[i], y=state.y[i], theta=state.theta[i],\n",
        "            dx=state.dx[i], dy=state.dy[i], omega=state.omega[i],\n",
        "            throttle=state.throttle[i], gimbal=state.gimbal[i],\n",
        "            fuel=state.fuel[i], target_angle=state.target_angle[i], time=state.time[i],\n",
        "        )\n",
        "        states.append(s)\n",
        "        if done[i]:\n",
        "            break\n",
        "    \n",
        "    # Only sum rewards up to the terminal step (after that, rewards are garbage due to r=0)\n",
        "    episode_length = len(states)\n",
        "    total_reward = float(jnp.sum(reward[:episode_length]))\n",
        "    print(f\"  Total reward: {total_reward:.2f}, Steps: {episode_length}\")\n",
        "    visualize_trajectory(states, env_params, title=f\"{title} (R={total_reward:.1f})\")\n",
        "\n",
        "print(\"âœ… Training function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ® Run Training!\n",
        "\n",
        "Adjust the parameters below and run training. With GPU, this should be quite fast!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the agent!\n",
        "# Adjust num_iters for longer/shorter training\n",
        "\n",
        "params, history = train(\n",
        "    num_iters=10000,          # Number of training iterations\n",
        "    steps_in_episode=1500,    # Max steps per episode\n",
        "    lr=0.0002,                # Learning rate\n",
        "    gamma=0.995,              # Discount factor\n",
        "    n_batches=128,            # Parallel episodes per iteration\n",
        "    hidden_size=256,          # Hidden layer size\n",
        "    n_hidden_layers=3,        # Number of hidden layers\n",
        "    visualize_every=1000,     # Visualize every N iterations\n",
        "    seed=42,\n",
        "    grad_clip=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Trained Policy\n",
        "\n",
        "Run multiple episodes to see how the trained agent performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained policy multiple times\n",
        "key = jr.PRNGKey(123)\n",
        "\n",
        "for i in range(5):\n",
        "    key, subkey = jr.split(key)\n",
        "    print(f\"\\n--- Episode {i+1} ---\")\n",
        "    visualize_policy(params, subkey, title=f\"Test Episode {i+1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save/Load Model\n",
        "\n",
        "You can save the trained parameters to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model (optional)\n",
        "import pickle\n",
        "\n",
        "# Flatten params for saving\n",
        "params_np = jax.tree.map(lambda x: np.array(x), params)\n",
        "\n",
        "with open('rocket_lander_params.pkl', 'wb') as f:\n",
        "    pickle.dump(params_np, f)\n",
        "\n",
        "print(\"Model saved to rocket_lander_params.pkl\")\n",
        "\n",
        "# To load:\n",
        "# with open('rocket_lander_params.pkl', 'rb') as f:\n",
        "#     params_loaded = pickle.load(f)\n",
        "# params_loaded = jax.tree.map(lambda x: jnp.array(x), params_loaded)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
